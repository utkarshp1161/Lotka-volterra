{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# non-linear pde- prey predator model - lotka volterra\n",
    " dx/dt = ax - bxy\n",
    "\n",
    " dy/dt = cxy - dy              initial cond : x= 2000, y=6000, find y and x\n",
    "\n",
    " a = 0.1\n",
    "\n",
    " b = 0.4\n",
    "\n",
    " c = 0.1\n",
    " \n",
    " d = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net_x(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_x, self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(1,5)\n",
    "        self.hidden_layer2 = nn.Linear(5,5)\n",
    "        self.hidden_layer3 = nn.Linear(5,5)\n",
    "        self.hidden_layer4 = nn.Linear(5,5)\n",
    "        self.hidden_layer5 = nn.Linear(5,5)\n",
    "        self.output_layer = nn.Linear(5,1)\n",
    "\n",
    "    def forward(self,t):\n",
    "        inputs = torch.cat([t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns\n",
    "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
    "        layer3_out = torch.sigmoid(self.hidden_layer3(layer2_out))\n",
    "        layer4_out = torch.sigmoid(self.hidden_layer4(layer3_out))\n",
    "        layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))\n",
    "        output = self.output_layer(layer5_out) ## For regression, no activation is used in output layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net_y(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_y, self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(1,5)\n",
    "        self.hidden_layer2 = nn.Linear(5,5)\n",
    "        self.hidden_layer3 = nn.Linear(5,5)\n",
    "        self.hidden_layer4 = nn.Linear(5,5)\n",
    "        self.hidden_layer5 = nn.Linear(5,5)\n",
    "        self.output_layer = nn.Linear(5,1)\n",
    "\n",
    "    def forward(self, t):\n",
    "        inputs = torch.cat([t],axis=1) # combined two arrays of 1 columns each to one array of 2 columns\n",
    "        layer1_out = torch.sigmoid(self.hidden_layer1(inputs))\n",
    "        layer2_out = torch.sigmoid(self.hidden_layer2(layer1_out))\n",
    "        layer3_out = torch.sigmoid(self.hidden_layer3(layer2_out))\n",
    "        layer4_out = torch.sigmoid(self.hidden_layer4(layer3_out))\n",
    "        layer5_out = torch.sigmoid(self.hidden_layer5(layer4_out))\n",
    "        output = self.output_layer(layer5_out) ## For regression, no activation is used in output layer\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (2) Model\n",
    "net_x = Net_x()\n",
    "net_y = Net_y()\n",
    "net_x = net_x.to(device)\n",
    "net_y = net_y.to(device)\n",
    "mse_cost_function = torch.nn.MSELoss() # Mean squared error\n",
    "optimizer = torch.optim.Adam(list(net_x.parameters())+ list(net_y.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PDE as loss function. Thus would use the network which we call as u_theta\n",
    "def f(t, net_x, net_y):\n",
    "    u = net_x(t) # the dependent variable u is given by the network based on independent variables x,t\n",
    "    v = net_y(t)\n",
    "    \n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True)[0]\n",
    "    v_t = torch.autograd.grad(v.sum(), t, create_graph=True)[0]\n",
    "    \n",
    "\n",
    "    pde_1 = u_t - 0.1*u + 0.4*u*v\n",
    "    pde_2 =v_t - 0.1*u*v + 0.4*v\n",
    "    \n",
    "    return pde_1, pde_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b.c ==> u(0) = 2000   and v(0) = 6000\n",
    "t_bc = np.zeros((500,1))\n",
    "u_bc = 2000*np.ones((500,1))\n",
    "v_bc = 6000*np.ones((500,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.],\n",
       "       [2000.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Traning Loss: tensor(38199424.)\n",
      "2 Traning Loss: tensor(38199320.)\n",
      "3 Traning Loss: tensor(38199240.)\n",
      "4 Traning Loss: tensor(38199152.)\n",
      "5 Traning Loss: tensor(38199056.)\n",
      "6 Traning Loss: tensor(38198976.)\n",
      "7 Traning Loss: tensor(38198876.)\n",
      "8 Traning Loss: tensor(38198792.)\n",
      "9 Traning Loss: tensor(38198700.)\n",
      "10 Traning Loss: tensor(38198616.)\n",
      "11 Traning Loss: tensor(38198536.)\n",
      "12 Traning Loss: tensor(38241080.)\n",
      "13 Traning Loss: tensor(38198348.)\n",
      "14 Traning Loss: tensor(38198260.)\n",
      "15 Traning Loss: tensor(38198172.)\n",
      "16 Traning Loss: tensor(38198092.)\n",
      "17 Traning Loss: tensor(38198004.)\n",
      "18 Traning Loss: tensor(38197908.)\n",
      "19 Traning Loss: tensor(38197820.)\n",
      "20 Traning Loss: tensor(38197732.)\n",
      "21 Traning Loss: tensor(38197652.)\n",
      "22 Traning Loss: tensor(38197540.)\n",
      "23 Traning Loss: tensor(38197460.)\n",
      "24 Traning Loss: tensor(38197372.)\n",
      "25 Traning Loss: tensor(38197284.)\n",
      "26 Traning Loss: tensor(38197196.)\n",
      "27 Traning Loss: tensor(38197116.)\n",
      "28 Traning Loss: tensor(38197016.)\n",
      "29 Traning Loss: tensor(38196924.)\n",
      "30 Traning Loss: tensor(38196836.)\n",
      "31 Traning Loss: tensor(38240276.)\n",
      "32 Traning Loss: tensor(38196660.)\n",
      "33 Traning Loss: tensor(38196572.)\n",
      "34 Traning Loss: tensor(38196484.)\n",
      "35 Traning Loss: tensor(38196400.)\n",
      "36 Traning Loss: tensor(38196316.)\n",
      "37 Traning Loss: tensor(38196208.)\n",
      "38 Traning Loss: tensor(38196124.)\n",
      "39 Traning Loss: tensor(38196036.)\n",
      "40 Traning Loss: tensor(38239944.)\n",
      "41 Traning Loss: tensor(38195872.)\n",
      "42 Traning Loss: tensor(38195772.)\n",
      "43 Traning Loss: tensor(38195684.)\n",
      "44 Traning Loss: tensor(38195600.)\n",
      "45 Traning Loss: tensor(38195512.)\n",
      "46 Traning Loss: tensor(38195424.)\n",
      "47 Traning Loss: tensor(38195340.)\n",
      "48 Traning Loss: tensor(38195244.)\n",
      "49 Traning Loss: tensor(38195160.)\n",
      "50 Traning Loss: tensor(38195072.)\n",
      "51 Traning Loss: tensor(38194988.)\n",
      "52 Traning Loss: tensor(38194904.)\n",
      "53 Traning Loss: tensor(38194800.)\n",
      "54 Traning Loss: tensor(38194716.)\n",
      "55 Traning Loss: tensor(38194616.)\n",
      "56 Traning Loss: tensor(38226612.)\n",
      "57 Traning Loss: tensor(38194456.)\n",
      "58 Traning Loss: tensor(38194360.)\n",
      "59 Traning Loss: tensor(38194280.)\n",
      "60 Traning Loss: tensor(38194196.)\n",
      "61 Traning Loss: tensor(38194120.)\n",
      "62 Traning Loss: tensor(38194040.)\n",
      "63 Traning Loss: tensor(38193948.)\n",
      "64 Traning Loss: tensor(38193872.)\n",
      "65 Traning Loss: tensor(38193792.)\n",
      "66 Traning Loss: tensor(38193712.)\n",
      "67 Traning Loss: tensor(38193640.)\n",
      "68 Traning Loss: tensor(38193544.)\n",
      "69 Traning Loss: tensor(38193464.)\n",
      "70 Traning Loss: tensor(38193504.)\n",
      "71 Traning Loss: tensor(38193300.)\n",
      "72 Traning Loss: tensor(38193220.)\n",
      "73 Traning Loss: tensor(38193132.)\n",
      "74 Traning Loss: tensor(38193048.)\n",
      "75 Traning Loss: tensor(38192960.)\n",
      "76 Traning Loss: tensor(38192880.)\n",
      "77 Traning Loss: tensor(38192792.)\n",
      "78 Traning Loss: tensor(38192704.)\n",
      "79 Traning Loss: tensor(38192616.)\n",
      "80 Traning Loss: tensor(38192528.)\n",
      "81 Traning Loss: tensor(38192444.)\n",
      "82 Traning Loss: tensor(38192340.)\n",
      "83 Traning Loss: tensor(38192256.)\n",
      "84 Traning Loss: tensor(38192176.)\n",
      "85 Traning Loss: tensor(38192072.)\n",
      "86 Traning Loss: tensor(38192000.)\n",
      "87 Traning Loss: tensor(38191900.)\n",
      "88 Traning Loss: tensor(38191800.)\n",
      "89 Traning Loss: tensor(38191712.)\n",
      "90 Traning Loss: tensor(38191624.)\n",
      "91 Traning Loss: tensor(38191536.)\n",
      "92 Traning Loss: tensor(38191436.)\n",
      "93 Traning Loss: tensor(38191348.)\n",
      "94 Traning Loss: tensor(38191256.)\n",
      "95 Traning Loss: tensor(38191164.)\n",
      "96 Traning Loss: tensor(38191080.)\n",
      "97 Traning Loss: tensor(38190992.)\n",
      "98 Traning Loss: tensor(38190888.)\n",
      "99 Traning Loss: tensor(38190792.)\n",
      "100 Traning Loss: tensor(38190700.)\n",
      "101 Traning Loss: tensor(38190628.)\n",
      "102 Traning Loss: tensor(38190520.)\n",
      "103 Traning Loss: tensor(38190428.)\n",
      "104 Traning Loss: tensor(38190340.)\n",
      "105 Traning Loss: tensor(38190256.)\n",
      "106 Traning Loss: tensor(38190156.)\n",
      "107 Traning Loss: tensor(38190080.)\n",
      "108 Traning Loss: tensor(38189976.)\n",
      "109 Traning Loss: tensor(38189884.)\n",
      "110 Traning Loss: tensor(38189796.)\n",
      "111 Traning Loss: tensor(38189704.)\n",
      "112 Traning Loss: tensor(38189604.)\n",
      "113 Traning Loss: tensor(38189520.)\n",
      "114 Traning Loss: tensor(38189424.)\n",
      "115 Traning Loss: tensor(38189344.)\n",
      "116 Traning Loss: tensor(38189248.)\n",
      "117 Traning Loss: tensor(38189156.)\n",
      "118 Traning Loss: tensor(38189056.)\n",
      "119 Traning Loss: tensor(38188980.)\n",
      "120 Traning Loss: tensor(38188884.)\n",
      "121 Traning Loss: tensor(38188796.)\n",
      "122 Traning Loss: tensor(38188696.)\n",
      "123 Traning Loss: tensor(38188612.)\n",
      "124 Traning Loss: tensor(38230324.)\n",
      "125 Traning Loss: tensor(38188444.)\n",
      "126 Traning Loss: tensor(38188360.)\n",
      "127 Traning Loss: tensor(38188280.)\n",
      "128 Traning Loss: tensor(38188184.)\n",
      "129 Traning Loss: tensor(38188100.)\n",
      "130 Traning Loss: tensor(38188020.)\n",
      "131 Traning Loss: tensor(38187936.)\n",
      "132 Traning Loss: tensor(38187840.)\n",
      "133 Traning Loss: tensor(38187748.)\n",
      "134 Traning Loss: tensor(38187672.)\n",
      "135 Traning Loss: tensor(38187584.)\n",
      "136 Traning Loss: tensor(38187492.)\n",
      "137 Traning Loss: tensor(38187412.)\n",
      "138 Traning Loss: tensor(38187308.)\n",
      "139 Traning Loss: tensor(38187220.)\n",
      "140 Traning Loss: tensor(38187132.)\n",
      "141 Traning Loss: tensor(38187044.)\n",
      "142 Traning Loss: tensor(38186948.)\n",
      "143 Traning Loss: tensor(38186856.)\n",
      "144 Traning Loss: tensor(38186772.)\n",
      "145 Traning Loss: tensor(38186684.)\n",
      "146 Traning Loss: tensor(38186596.)\n",
      "147 Traning Loss: tensor(38186500.)\n",
      "148 Traning Loss: tensor(38186416.)\n",
      "149 Traning Loss: tensor(38186332.)\n",
      "150 Traning Loss: tensor(38186232.)\n",
      "151 Traning Loss: tensor(38186148.)\n",
      "152 Traning Loss: tensor(38186044.)\n",
      "153 Traning Loss: tensor(38185960.)\n",
      "154 Traning Loss: tensor(38185872.)\n",
      "155 Traning Loss: tensor(38185780.)\n",
      "156 Traning Loss: tensor(38185700.)\n",
      "157 Traning Loss: tensor(38185592.)\n",
      "158 Traning Loss: tensor(38185508.)\n",
      "159 Traning Loss: tensor(38185424.)\n",
      "160 Traning Loss: tensor(38185332.)\n",
      "161 Traning Loss: tensor(38185232.)\n",
      "162 Traning Loss: tensor(38185144.)\n",
      "163 Traning Loss: tensor(38185060.)\n",
      "164 Traning Loss: tensor(38184960.)\n",
      "165 Traning Loss: tensor(38184876.)\n",
      "166 Traning Loss: tensor(38184792.)\n",
      "167 Traning Loss: tensor(38184704.)\n",
      "168 Traning Loss: tensor(38184604.)\n",
      "169 Traning Loss: tensor(38184512.)\n",
      "170 Traning Loss: tensor(38184428.)\n",
      "171 Traning Loss: tensor(38184328.)\n",
      "172 Traning Loss: tensor(38184240.)\n",
      "173 Traning Loss: tensor(38218104.)\n",
      "174 Traning Loss: tensor(38184084.)\n",
      "175 Traning Loss: tensor(38184012.)\n",
      "176 Traning Loss: tensor(38183956.)\n",
      "177 Traning Loss: tensor(38183880.)\n",
      "178 Traning Loss: tensor(38183820.)\n",
      "179 Traning Loss: tensor(38183748.)\n",
      "180 Traning Loss: tensor(38183700.)\n",
      "181 Traning Loss: tensor(38183632.)\n",
      "182 Traning Loss: tensor(38183552.)\n",
      "183 Traning Loss: tensor(38183492.)\n",
      "184 Traning Loss: tensor(38183424.)\n",
      "185 Traning Loss: tensor(38183356.)\n",
      "186 Traning Loss: tensor(38183284.)\n",
      "187 Traning Loss: tensor(38183196.)\n",
      "188 Traning Loss: tensor(38183112.)\n",
      "189 Traning Loss: tensor(38183028.)\n",
      "190 Traning Loss: tensor(38182940.)\n",
      "191 Traning Loss: tensor(38182832.)\n",
      "192 Traning Loss: tensor(38182740.)\n",
      "193 Traning Loss: tensor(38182648.)\n",
      "194 Traning Loss: tensor(38182540.)\n",
      "195 Traning Loss: tensor(38182452.)\n",
      "196 Traning Loss: tensor(38182352.)\n",
      "197 Traning Loss: tensor(38182232.)\n",
      "198 Traning Loss: tensor(38227168.)\n",
      "199 Traning Loss: tensor(38182040.)\n",
      "200 Traning Loss: tensor(38181944.)\n",
      "201 Traning Loss: tensor(38181852.)\n",
      "202 Traning Loss: tensor(38181740.)\n",
      "203 Traning Loss: tensor(38181644.)\n",
      "204 Traning Loss: tensor(38181548.)\n",
      "205 Traning Loss: tensor(38181456.)\n",
      "206 Traning Loss: tensor(38181360.)\n",
      "207 Traning Loss: tensor(38181248.)\n",
      "208 Traning Loss: tensor(38181156.)\n",
      "209 Traning Loss: tensor(38181064.)\n",
      "210 Traning Loss: tensor(38180972.)\n",
      "211 Traning Loss: tensor(38180876.)\n",
      "212 Traning Loss: tensor(38180764.)\n",
      "213 Traning Loss: tensor(38180672.)\n",
      "214 Traning Loss: tensor(38180588.)\n",
      "215 Traning Loss: tensor(38180492.)\n",
      "216 Traning Loss: tensor(38180408.)\n",
      "217 Traning Loss: tensor(38180300.)\n",
      "218 Traning Loss: tensor(38180212.)\n",
      "219 Traning Loss: tensor(38180120.)\n",
      "220 Traning Loss: tensor(38180024.)\n",
      "221 Traning Loss: tensor(38179920.)\n",
      "222 Traning Loss: tensor(38179840.)\n",
      "223 Traning Loss: tensor(38179748.)\n",
      "224 Traning Loss: tensor(38179656.)\n",
      "225 Traning Loss: tensor(38179568.)\n",
      "226 Traning Loss: tensor(38179480.)\n",
      "227 Traning Loss: tensor(38179372.)\n",
      "228 Traning Loss: tensor(38179284.)\n",
      "229 Traning Loss: tensor(38179196.)\n",
      "230 Traning Loss: tensor(38179108.)\n",
      "231 Traning Loss: tensor(38178996.)\n",
      "232 Traning Loss: tensor(38178920.)\n",
      "233 Traning Loss: tensor(38222380.)\n",
      "234 Traning Loss: tensor(38178748.)\n",
      "235 Traning Loss: tensor(38178660.)\n",
      "236 Traning Loss: tensor(38178580.)\n",
      "237 Traning Loss: tensor(38178480.)\n",
      "238 Traning Loss: tensor(38178396.)\n",
      "239 Traning Loss: tensor(38178316.)\n",
      "240 Traning Loss: tensor(38178232.)\n",
      "241 Traning Loss: tensor(38178144.)\n",
      "242 Traning Loss: tensor(38178044.)\n",
      "243 Traning Loss: tensor(38177964.)\n",
      "244 Traning Loss: tensor(38177864.)\n",
      "245 Traning Loss: tensor(38222872.)\n",
      "246 Traning Loss: tensor(38177700.)\n",
      "247 Traning Loss: tensor(38177596.)\n",
      "248 Traning Loss: tensor(38179184.)\n",
      "249 Traning Loss: tensor(38177420.)\n",
      "250 Traning Loss: tensor(38177332.)\n",
      "251 Traning Loss: tensor(38177240.)\n",
      "252 Traning Loss: tensor(38177132.)\n",
      "253 Traning Loss: tensor(38177044.)\n",
      "254 Traning Loss: tensor(38176956.)\n",
      "255 Traning Loss: tensor(38176864.)\n",
      "256 Traning Loss: tensor(38176780.)\n",
      "257 Traning Loss: tensor(38176676.)\n",
      "258 Traning Loss: tensor(38176592.)\n",
      "259 Traning Loss: tensor(38176488.)\n",
      "260 Traning Loss: tensor(38176404.)\n",
      "261 Traning Loss: tensor(38176320.)\n",
      "262 Traning Loss: tensor(38176228.)\n",
      "263 Traning Loss: tensor(38176136.)\n",
      "264 Traning Loss: tensor(38176040.)\n",
      "265 Traning Loss: tensor(38175956.)\n",
      "266 Traning Loss: tensor(38175864.)\n",
      "267 Traning Loss: tensor(38175772.)\n",
      "268 Traning Loss: tensor(38175684.)\n",
      "269 Traning Loss: tensor(38175596.)\n",
      "270 Traning Loss: tensor(38175508.)\n",
      "271 Traning Loss: tensor(38175404.)\n",
      "272 Traning Loss: tensor(38175320.)\n",
      "273 Traning Loss: tensor(38175236.)\n",
      "274 Traning Loss: tensor(38175136.)\n",
      "275 Traning Loss: tensor(38175064.)\n",
      "276 Traning Loss: tensor(38174968.)\n",
      "277 Traning Loss: tensor(38174868.)\n",
      "278 Traning Loss: tensor(38174784.)\n",
      "279 Traning Loss: tensor(38174696.)\n",
      "280 Traning Loss: tensor(38174612.)\n",
      "281 Traning Loss: tensor(38174508.)\n",
      "282 Traning Loss: tensor(38174424.)\n",
      "283 Traning Loss: tensor(38174340.)\n",
      "284 Traning Loss: tensor(38174248.)\n",
      "285 Traning Loss: tensor(38174164.)\n",
      "286 Traning Loss: tensor(38174076.)\n",
      "287 Traning Loss: tensor(38173980.)\n",
      "288 Traning Loss: tensor(38173892.)\n",
      "289 Traning Loss: tensor(38173796.)\n",
      "290 Traning Loss: tensor(38173720.)\n",
      "291 Traning Loss: tensor(38173616.)\n",
      "292 Traning Loss: tensor(38173524.)\n",
      "293 Traning Loss: tensor(38173444.)\n",
      "294 Traning Loss: tensor(38173364.)\n",
      "295 Traning Loss: tensor(38173264.)\n",
      "296 Traning Loss: tensor(38173184.)\n",
      "297 Traning Loss: tensor(38173084.)\n",
      "298 Traning Loss: tensor(38172992.)\n",
      "299 Traning Loss: tensor(38172908.)\n",
      "300 Traning Loss: tensor(38172828.)\n",
      "301 Traning Loss: tensor(38172724.)\n",
      "302 Traning Loss: tensor(38172644.)\n",
      "303 Traning Loss: tensor(38172556.)\n",
      "304 Traning Loss: tensor(38172468.)\n",
      "305 Traning Loss: tensor(38172372.)\n",
      "306 Traning Loss: tensor(38172284.)\n",
      "307 Traning Loss: tensor(38172188.)\n",
      "308 Traning Loss: tensor(38172100.)\n",
      "309 Traning Loss: tensor(38218440.)\n",
      "310 Traning Loss: tensor(38171928.)\n",
      "311 Traning Loss: tensor(38171828.)\n",
      "312 Traning Loss: tensor(38171744.)\n",
      "313 Traning Loss: tensor(38171660.)\n",
      "314 Traning Loss: tensor(38171572.)\n",
      "315 Traning Loss: tensor(38171488.)\n",
      "316 Traning Loss: tensor(38171392.)\n",
      "317 Traning Loss: tensor(38171296.)\n",
      "318 Traning Loss: tensor(38171216.)\n",
      "319 Traning Loss: tensor(38171128.)\n",
      "320 Traning Loss: tensor(38171036.)\n",
      "321 Traning Loss: tensor(38170944.)\n",
      "322 Traning Loss: tensor(38170848.)\n",
      "323 Traning Loss: tensor(38170776.)\n",
      "324 Traning Loss: tensor(38170676.)\n",
      "325 Traning Loss: tensor(38170596.)\n",
      "326 Traning Loss: tensor(38170512.)\n",
      "327 Traning Loss: tensor(38170408.)\n",
      "328 Traning Loss: tensor(38170320.)\n",
      "329 Traning Loss: tensor(38170236.)\n",
      "330 Traning Loss: tensor(38170152.)\n",
      "331 Traning Loss: tensor(38170052.)\n",
      "332 Traning Loss: tensor(38169964.)\n",
      "333 Traning Loss: tensor(38169880.)\n",
      "334 Traning Loss: tensor(38169788.)\n",
      "335 Traning Loss: tensor(38169700.)\n",
      "336 Traning Loss: tensor(38169604.)\n",
      "337 Traning Loss: tensor(38169520.)\n",
      "338 Traning Loss: tensor(38169432.)\n",
      "339 Traning Loss: tensor(38169336.)\n",
      "340 Traning Loss: tensor(38169252.)\n",
      "341 Traning Loss: tensor(38169156.)\n",
      "342 Traning Loss: tensor(38169072.)\n",
      "343 Traning Loss: tensor(38168984.)\n",
      "344 Traning Loss: tensor(38168896.)\n",
      "345 Traning Loss: tensor(38207888.)\n",
      "346 Traning Loss: tensor(38168712.)\n",
      "347 Traning Loss: tensor(38168636.)\n",
      "348 Traning Loss: tensor(38168560.)\n",
      "349 Traning Loss: tensor(38168476.)\n",
      "350 Traning Loss: tensor(38168396.)\n",
      "351 Traning Loss: tensor(38168300.)\n",
      "352 Traning Loss: tensor(38168224.)\n",
      "353 Traning Loss: tensor(38168136.)\n",
      "354 Traning Loss: tensor(38168048.)\n",
      "355 Traning Loss: tensor(38167972.)\n",
      "356 Traning Loss: tensor(38167892.)\n",
      "357 Traning Loss: tensor(38167800.)\n",
      "358 Traning Loss: tensor(38167712.)\n",
      "359 Traning Loss: tensor(38167636.)\n",
      "360 Traning Loss: tensor(38167556.)\n",
      "361 Traning Loss: tensor(38167456.)\n",
      "362 Traning Loss: tensor(38167368.)\n",
      "363 Traning Loss: tensor(38167288.)\n",
      "364 Traning Loss: tensor(38167200.)\n",
      "365 Traning Loss: tensor(38167116.)\n",
      "366 Traning Loss: tensor(38167016.)\n",
      "367 Traning Loss: tensor(38166932.)\n",
      "368 Traning Loss: tensor(38166844.)\n",
      "369 Traning Loss: tensor(38166760.)\n",
      "370 Traning Loss: tensor(38166668.)\n",
      "371 Traning Loss: tensor(38166564.)\n",
      "372 Traning Loss: tensor(38166480.)\n",
      "373 Traning Loss: tensor(38166388.)\n",
      "374 Traning Loss: tensor(38166308.)\n",
      "375 Traning Loss: tensor(38166220.)\n",
      "376 Traning Loss: tensor(38166120.)\n",
      "377 Traning Loss: tensor(38166028.)\n",
      "378 Traning Loss: tensor(38165944.)\n",
      "379 Traning Loss: tensor(38165852.)\n",
      "380 Traning Loss: tensor(38165764.)\n",
      "381 Traning Loss: tensor(38165668.)\n",
      "382 Traning Loss: tensor(38165580.)\n",
      "383 Traning Loss: tensor(38165488.)\n",
      "384 Traning Loss: tensor(38165404.)\n",
      "385 Traning Loss: tensor(38165316.)\n",
      "386 Traning Loss: tensor(38165204.)\n",
      "387 Traning Loss: tensor(38165116.)\n",
      "388 Traning Loss: tensor(38165032.)\n",
      "389 Traning Loss: tensor(38164948.)\n",
      "390 Traning Loss: tensor(38164844.)\n",
      "391 Traning Loss: tensor(38164760.)\n",
      "392 Traning Loss: tensor(38164668.)\n",
      "393 Traning Loss: tensor(38164576.)\n",
      "394 Traning Loss: tensor(38164492.)\n",
      "395 Traning Loss: tensor(38181984.)\n",
      "396 Traning Loss: tensor(38165384.)\n",
      "397 Traning Loss: tensor(38164216.)\n",
      "398 Traning Loss: tensor(38164132.)\n",
      "399 Traning Loss: tensor(38164044.)\n",
      "400 Traning Loss: tensor(38163932.)\n",
      "401 Traning Loss: tensor(38163852.)\n",
      "402 Traning Loss: tensor(38163760.)\n",
      "403 Traning Loss: tensor(38163672.)\n",
      "404 Traning Loss: tensor(38163588.)\n",
      "405 Traning Loss: tensor(38163504.)\n",
      "406 Traning Loss: tensor(38163400.)\n",
      "407 Traning Loss: tensor(38163312.)\n",
      "408 Traning Loss: tensor(38163224.)\n",
      "409 Traning Loss: tensor(38163120.)\n",
      "410 Traning Loss: tensor(38163036.)\n",
      "411 Traning Loss: tensor(38162952.)\n",
      "412 Traning Loss: tensor(38162868.)\n",
      "413 Traning Loss: tensor(38162780.)\n",
      "414 Traning Loss: tensor(38162688.)\n",
      "415 Traning Loss: tensor(38162608.)\n",
      "416 Traning Loss: tensor(38162496.)\n",
      "417 Traning Loss: tensor(38162408.)\n",
      "418 Traning Loss: tensor(38162324.)\n",
      "419 Traning Loss: tensor(38162224.)\n",
      "420 Traning Loss: tensor(38162144.)\n",
      "421 Traning Loss: tensor(38162056.)\n",
      "422 Traning Loss: tensor(38161964.)\n",
      "423 Traning Loss: tensor(38161880.)\n",
      "424 Traning Loss: tensor(38208580.)\n",
      "425 Traning Loss: tensor(38161696.)\n",
      "426 Traning Loss: tensor(38161608.)\n",
      "427 Traning Loss: tensor(38161524.)\n",
      "428 Traning Loss: tensor(38161440.)\n",
      "429 Traning Loss: tensor(38161352.)\n",
      "430 Traning Loss: tensor(38161256.)\n",
      "431 Traning Loss: tensor(38161160.)\n",
      "432 Traning Loss: tensor(38161076.)\n",
      "433 Traning Loss: tensor(38160984.)\n",
      "434 Traning Loss: tensor(38160908.)\n",
      "435 Traning Loss: tensor(38160804.)\n",
      "436 Traning Loss: tensor(38160720.)\n",
      "437 Traning Loss: tensor(38160632.)\n",
      "438 Traning Loss: tensor(38160548.)\n",
      "439 Traning Loss: tensor(38160440.)\n",
      "440 Traning Loss: tensor(38160356.)\n",
      "441 Traning Loss: tensor(38160272.)\n",
      "442 Traning Loss: tensor(38160184.)\n",
      "443 Traning Loss: tensor(38160096.)\n",
      "444 Traning Loss: tensor(38160016.)\n",
      "445 Traning Loss: tensor(38159904.)\n",
      "446 Traning Loss: tensor(38159824.)\n",
      "447 Traning Loss: tensor(38159740.)\n",
      "448 Traning Loss: tensor(38171620.)\n",
      "449 Traning Loss: tensor(38159572.)\n",
      "450 Traning Loss: tensor(38159516.)\n",
      "451 Traning Loss: tensor(38159476.)\n",
      "452 Traning Loss: tensor(38159440.)\n",
      "453 Traning Loss: tensor(38159420.)\n",
      "454 Traning Loss: tensor(38159408.)\n",
      "455 Traning Loss: tensor(38159396.)\n",
      "456 Traning Loss: tensor(38159408.)\n",
      "457 Traning Loss: tensor(38159412.)\n",
      "458 Traning Loss: tensor(38199156.)\n",
      "459 Traning Loss: tensor(38159604.)\n",
      "460 Traning Loss: tensor(38159784.)\n",
      "461 Traning Loss: tensor(38159996.)\n",
      "462 Traning Loss: tensor(38160148.)\n",
      "463 Traning Loss: tensor(38160216.)\n",
      "464 Traning Loss: tensor(38160188.)\n",
      "465 Traning Loss: tensor(38160032.)\n",
      "466 Traning Loss: tensor(38159796.)\n",
      "467 Traning Loss: tensor(38159512.)\n",
      "468 Traning Loss: tensor(38159212.)\n",
      "469 Traning Loss: tensor(38158916.)\n",
      "470 Traning Loss: tensor(38158624.)\n",
      "471 Traning Loss: tensor(38158376.)\n",
      "472 Traning Loss: tensor(38158148.)\n",
      "473 Traning Loss: tensor(38157956.)\n",
      "474 Traning Loss: tensor(38157784.)\n",
      "475 Traning Loss: tensor(38157624.)\n",
      "476 Traning Loss: tensor(38157464.)\n",
      "477 Traning Loss: tensor(38157324.)\n",
      "478 Traning Loss: tensor(38157196.)\n",
      "479 Traning Loss: tensor(38157084.)\n",
      "480 Traning Loss: tensor(38156964.)\n",
      "481 Traning Loss: tensor(38156852.)\n",
      "482 Traning Loss: tensor(38156740.)\n",
      "483 Traning Loss: tensor(38156644.)\n",
      "484 Traning Loss: tensor(38156540.)\n",
      "485 Traning Loss: tensor(38156436.)\n",
      "486 Traning Loss: tensor(38156340.)\n",
      "487 Traning Loss: tensor(38156244.)\n",
      "488 Traning Loss: tensor(38156152.)\n",
      "489 Traning Loss: tensor(38156060.)\n",
      "490 Traning Loss: tensor(38155952.)\n",
      "491 Traning Loss: tensor(38155868.)\n",
      "492 Traning Loss: tensor(38155772.)\n",
      "493 Traning Loss: tensor(38155688.)\n",
      "494 Traning Loss: tensor(38155588.)\n",
      "495 Traning Loss: tensor(38155496.)\n",
      "496 Traning Loss: tensor(38155400.)\n",
      "497 Traning Loss: tensor(38155308.)\n",
      "498 Traning Loss: tensor(38155224.)\n",
      "499 Traning Loss: tensor(38155136.)\n",
      "500 Traning Loss: tensor(38155032.)\n",
      "501 Traning Loss: tensor(38154948.)\n",
      "502 Traning Loss: tensor(38154856.)\n",
      "503 Traning Loss: tensor(38154772.)\n",
      "504 Traning Loss: tensor(38154684.)\n",
      "505 Traning Loss: tensor(38154580.)\n",
      "506 Traning Loss: tensor(38154492.)\n",
      "507 Traning Loss: tensor(38154408.)\n",
      "508 Traning Loss: tensor(38154316.)\n",
      "509 Traning Loss: tensor(38154220.)\n",
      "510 Traning Loss: tensor(38154136.)\n",
      "511 Traning Loss: tensor(38154040.)\n",
      "512 Traning Loss: tensor(38153952.)\n",
      "513 Traning Loss: tensor(38153864.)\n",
      "514 Traning Loss: tensor(38153780.)\n",
      "515 Traning Loss: tensor(38153676.)\n",
      "516 Traning Loss: tensor(38153592.)\n",
      "517 Traning Loss: tensor(38153508.)\n",
      "518 Traning Loss: tensor(38153420.)\n",
      "519 Traning Loss: tensor(38153320.)\n",
      "520 Traning Loss: tensor(38153236.)\n",
      "521 Traning Loss: tensor(38153148.)\n",
      "522 Traning Loss: tensor(38153060.)\n",
      "523 Traning Loss: tensor(38152964.)\n",
      "524 Traning Loss: tensor(38152880.)\n",
      "525 Traning Loss: tensor(38152788.)\n",
      "526 Traning Loss: tensor(38152692.)\n",
      "527 Traning Loss: tensor(38152608.)\n",
      "528 Traning Loss: tensor(38152524.)\n",
      "529 Traning Loss: tensor(38152424.)\n",
      "530 Traning Loss: tensor(38152336.)\n",
      "531 Traning Loss: tensor(38152256.)\n",
      "532 Traning Loss: tensor(38152164.)\n",
      "533 Traning Loss: tensor(38152072.)\n",
      "534 Traning Loss: tensor(38151988.)\n",
      "535 Traning Loss: tensor(38151888.)\n",
      "536 Traning Loss: tensor(38151804.)\n",
      "537 Traning Loss: tensor(38151716.)\n",
      "538 Traning Loss: tensor(38151620.)\n",
      "539 Traning Loss: tensor(38151528.)\n",
      "540 Traning Loss: tensor(38151436.)\n",
      "541 Traning Loss: tensor(38151360.)\n",
      "542 Traning Loss: tensor(38151264.)\n",
      "543 Traning Loss: tensor(38151180.)\n",
      "544 Traning Loss: tensor(38151084.)\n",
      "545 Traning Loss: tensor(38150996.)\n",
      "546 Traning Loss: tensor(38150912.)\n",
      "547 Traning Loss: tensor(38150820.)\n",
      "548 Traning Loss: tensor(38150736.)\n",
      "549 Traning Loss: tensor(38150640.)\n",
      "550 Traning Loss: tensor(38150552.)\n",
      "551 Traning Loss: tensor(38150464.)\n",
      "552 Traning Loss: tensor(38150376.)\n",
      "553 Traning Loss: tensor(38150284.)\n",
      "554 Traning Loss: tensor(38150188.)\n",
      "555 Traning Loss: tensor(38150104.)\n",
      "556 Traning Loss: tensor(38150020.)\n",
      "557 Traning Loss: tensor(38149924.)\n",
      "558 Traning Loss: tensor(38149840.)\n",
      "559 Traning Loss: tensor(38149736.)\n",
      "560 Traning Loss: tensor(38149656.)\n",
      "561 Traning Loss: tensor(38149568.)\n",
      "562 Traning Loss: tensor(38196532.)\n",
      "563 Traning Loss: tensor(38149404.)\n",
      "564 Traning Loss: tensor(38149320.)\n",
      "565 Traning Loss: tensor(38149220.)\n",
      "566 Traning Loss: tensor(38149140.)\n",
      "567 Traning Loss: tensor(38149048.)\n",
      "568 Traning Loss: tensor(38148972.)\n",
      "569 Traning Loss: tensor(38185380.)\n",
      "570 Traning Loss: tensor(38148804.)\n",
      "571 Traning Loss: tensor(38148740.)\n",
      "572 Traning Loss: tensor(38148664.)\n",
      "573 Traning Loss: tensor(38148624.)\n",
      "574 Traning Loss: tensor(38148564.)\n",
      "575 Traning Loss: tensor(38148488.)\n",
      "576 Traning Loss: tensor(38148436.)\n",
      "577 Traning Loss: tensor(38148380.)\n",
      "578 Traning Loss: tensor(38148332.)\n",
      "579 Traning Loss: tensor(38148264.)\n",
      "580 Traning Loss: tensor(38148212.)\n",
      "581 Traning Loss: tensor(38148152.)\n",
      "582 Traning Loss: tensor(38148084.)\n",
      "583 Traning Loss: tensor(38148012.)\n",
      "584 Traning Loss: tensor(38147936.)\n",
      "585 Traning Loss: tensor(38147840.)\n",
      "586 Traning Loss: tensor(38147756.)\n",
      "587 Traning Loss: tensor(38147668.)\n",
      "588 Traning Loss: tensor(38147576.)\n",
      "589 Traning Loss: tensor(38147480.)\n",
      "590 Traning Loss: tensor(38147360.)\n",
      "591 Traning Loss: tensor(38147256.)\n",
      "592 Traning Loss: tensor(38147160.)\n",
      "593 Traning Loss: tensor(38147052.)\n",
      "594 Traning Loss: tensor(38182508.)\n",
      "595 Traning Loss: tensor(38146944.)\n",
      "596 Traning Loss: tensor(38146956.)\n",
      "597 Traning Loss: tensor(38146976.)\n",
      "598 Traning Loss: tensor(38146988.)\n",
      "599 Traning Loss: tensor(38146984.)\n",
      "600 Traning Loss: tensor(38146976.)\n",
      "601 Traning Loss: tensor(38146948.)\n",
      "602 Traning Loss: tensor(38146888.)\n",
      "603 Traning Loss: tensor(38146804.)\n",
      "604 Traning Loss: tensor(38146704.)\n",
      "605 Traning Loss: tensor(38146580.)\n",
      "606 Traning Loss: tensor(38146416.)\n",
      "607 Traning Loss: tensor(38146272.)\n",
      "608 Traning Loss: tensor(38146112.)\n",
      "609 Traning Loss: tensor(38145948.)\n",
      "610 Traning Loss: tensor(38145800.)\n",
      "611 Traning Loss: tensor(38145656.)\n",
      "612 Traning Loss: tensor(38145512.)\n",
      "613 Traning Loss: tensor(38145372.)\n",
      "614 Traning Loss: tensor(38145244.)\n",
      "615 Traning Loss: tensor(38145104.)\n",
      "616 Traning Loss: tensor(38144984.)\n",
      "617 Traning Loss: tensor(38144864.)\n",
      "618 Traning Loss: tensor(38144756.)\n",
      "619 Traning Loss: tensor(38144648.)\n",
      "620 Traning Loss: tensor(38144516.)\n",
      "621 Traning Loss: tensor(38144428.)\n",
      "622 Traning Loss: tensor(38144316.)\n",
      "623 Traning Loss: tensor(38144216.)\n",
      "624 Traning Loss: tensor(38144120.)\n",
      "625 Traning Loss: tensor(38144008.)\n",
      "626 Traning Loss: tensor(38143916.)\n",
      "627 Traning Loss: tensor(38143816.)\n",
      "628 Traning Loss: tensor(38143836.)\n",
      "629 Traning Loss: tensor(38143628.)\n",
      "630 Traning Loss: tensor(38143524.)\n",
      "631 Traning Loss: tensor(38143436.)\n",
      "632 Traning Loss: tensor(38143340.)\n",
      "633 Traning Loss: tensor(38143252.)\n",
      "634 Traning Loss: tensor(38143168.)\n",
      "635 Traning Loss: tensor(38143052.)\n",
      "636 Traning Loss: tensor(38142972.)\n",
      "637 Traning Loss: tensor(38142872.)\n",
      "638 Traning Loss: tensor(38142784.)\n",
      "639 Traning Loss: tensor(38142680.)\n",
      "640 Traning Loss: tensor(38142604.)\n",
      "641 Traning Loss: tensor(38142508.)\n",
      "642 Traning Loss: tensor(38142420.)\n",
      "643 Traning Loss: tensor(38142332.)\n",
      "644 Traning Loss: tensor(38142240.)\n",
      "645 Traning Loss: tensor(38142140.)\n",
      "646 Traning Loss: tensor(38142052.)\n",
      "647 Traning Loss: tensor(38141964.)\n",
      "648 Traning Loss: tensor(38141876.)\n",
      "649 Traning Loss: tensor(38141772.)\n",
      "650 Traning Loss: tensor(38141692.)\n",
      "651 Traning Loss: tensor(38141608.)\n",
      "652 Traning Loss: tensor(38141508.)\n",
      "653 Traning Loss: tensor(38141424.)\n",
      "654 Traning Loss: tensor(38141336.)\n",
      "655 Traning Loss: tensor(38141236.)\n",
      "656 Traning Loss: tensor(38141144.)\n",
      "657 Traning Loss: tensor(38141060.)\n",
      "658 Traning Loss: tensor(38140980.)\n",
      "659 Traning Loss: tensor(38145028.)\n",
      "660 Traning Loss: tensor(38140796.)\n",
      "661 Traning Loss: tensor(38140720.)\n",
      "662 Traning Loss: tensor(38140636.)\n",
      "663 Traning Loss: tensor(38140564.)\n",
      "664 Traning Loss: tensor(38180384.)\n",
      "665 Traning Loss: tensor(38140412.)\n",
      "666 Traning Loss: tensor(38140352.)\n",
      "667 Traning Loss: tensor(38140308.)\n",
      "668 Traning Loss: tensor(38140268.)\n",
      "669 Traning Loss: tensor(38140220.)\n",
      "670 Traning Loss: tensor(38140180.)\n",
      "671 Traning Loss: tensor(38140140.)\n",
      "672 Traning Loss: tensor(38140104.)\n",
      "673 Traning Loss: tensor(38140056.)\n",
      "674 Traning Loss: tensor(38139992.)\n",
      "675 Traning Loss: tensor(38139936.)\n",
      "676 Traning Loss: tensor(38139872.)\n",
      "677 Traning Loss: tensor(38139796.)\n",
      "678 Traning Loss: tensor(38139708.)\n",
      "679 Traning Loss: tensor(38139604.)\n",
      "680 Traning Loss: tensor(38139512.)\n",
      "681 Traning Loss: tensor(38139396.)\n",
      "682 Traning Loss: tensor(38139300.)\n",
      "683 Traning Loss: tensor(38139192.)\n",
      "684 Traning Loss: tensor(38146012.)\n",
      "685 Traning Loss: tensor(38139044.)\n",
      "686 Traning Loss: tensor(38139028.)\n",
      "687 Traning Loss: tensor(38139008.)\n",
      "688 Traning Loss: tensor(38138968.)\n",
      "689 Traning Loss: tensor(38138908.)\n",
      "690 Traning Loss: tensor(38138840.)\n",
      "691 Traning Loss: tensor(38138760.)\n",
      "692 Traning Loss: tensor(38138656.)\n",
      "693 Traning Loss: tensor(38138548.)\n",
      "694 Traning Loss: tensor(38138436.)\n",
      "695 Traning Loss: tensor(38138292.)\n",
      "696 Traning Loss: tensor(38138168.)\n",
      "697 Traning Loss: tensor(38138036.)\n",
      "698 Traning Loss: tensor(38137904.)\n",
      "699 Traning Loss: tensor(38137760.)\n",
      "700 Traning Loss: tensor(38137624.)\n",
      "701 Traning Loss: tensor(38137512.)\n",
      "702 Traning Loss: tensor(38178944.)\n",
      "703 Traning Loss: tensor(38137340.)\n",
      "704 Traning Loss: tensor(38137296.)\n",
      "705 Traning Loss: tensor(38137228.)\n",
      "706 Traning Loss: tensor(38137172.)\n",
      "707 Traning Loss: tensor(38137104.)\n",
      "708 Traning Loss: tensor(38137032.)\n",
      "709 Traning Loss: tensor(38136940.)\n",
      "710 Traning Loss: tensor(38136848.)\n",
      "711 Traning Loss: tensor(38136752.)\n",
      "712 Traning Loss: tensor(38136644.)\n",
      "713 Traning Loss: tensor(38136544.)\n",
      "714 Traning Loss: tensor(38136432.)\n",
      "715 Traning Loss: tensor(38136308.)\n",
      "716 Traning Loss: tensor(38136200.)\n",
      "717 Traning Loss: tensor(38136076.)\n",
      "718 Traning Loss: tensor(38135968.)\n",
      "719 Traning Loss: tensor(38135840.)\n",
      "720 Traning Loss: tensor(38135736.)\n",
      "721 Traning Loss: tensor(38135632.)\n",
      "722 Traning Loss: tensor(38135520.)\n",
      "723 Traning Loss: tensor(38135416.)\n",
      "724 Traning Loss: tensor(38135312.)\n",
      "725 Traning Loss: tensor(38135192.)\n",
      "726 Traning Loss: tensor(38135096.)\n",
      "727 Traning Loss: tensor(38134988.)\n",
      "728 Traning Loss: tensor(38134896.)\n",
      "729 Traning Loss: tensor(38134784.)\n",
      "730 Traning Loss: tensor(38134688.)\n",
      "731 Traning Loss: tensor(38134596.)\n",
      "732 Traning Loss: tensor(38134488.)\n",
      "733 Traning Loss: tensor(38134396.)\n",
      "734 Traning Loss: tensor(38134304.)\n",
      "735 Traning Loss: tensor(38134196.)\n",
      "736 Traning Loss: tensor(38134100.)\n",
      "737 Traning Loss: tensor(38134012.)\n",
      "738 Traning Loss: tensor(38133924.)\n",
      "739 Traning Loss: tensor(38133820.)\n",
      "740 Traning Loss: tensor(38133728.)\n",
      "741 Traning Loss: tensor(38133636.)\n",
      "742 Traning Loss: tensor(38133544.)\n",
      "743 Traning Loss: tensor(38133456.)\n",
      "744 Traning Loss: tensor(38133352.)\n",
      "745 Traning Loss: tensor(38133264.)\n",
      "746 Traning Loss: tensor(38133176.)\n",
      "747 Traning Loss: tensor(38133084.)\n",
      "748 Traning Loss: tensor(38132996.)\n",
      "749 Traning Loss: tensor(38132888.)\n",
      "750 Traning Loss: tensor(38132808.)\n",
      "751 Traning Loss: tensor(38132716.)\n",
      "752 Traning Loss: tensor(38132632.)\n",
      "753 Traning Loss: tensor(38132540.)\n",
      "754 Traning Loss: tensor(38132440.)\n",
      "755 Traning Loss: tensor(38132352.)\n",
      "756 Traning Loss: tensor(38132264.)\n",
      "757 Traning Loss: tensor(38132176.)\n",
      "758 Traning Loss: tensor(38132088.)\n",
      "759 Traning Loss: tensor(38131984.)\n",
      "760 Traning Loss: tensor(38131900.)\n",
      "761 Traning Loss: tensor(38131808.)\n",
      "762 Traning Loss: tensor(38131724.)\n",
      "763 Traning Loss: tensor(38131636.)\n",
      "764 Traning Loss: tensor(38131528.)\n",
      "765 Traning Loss: tensor(38131440.)\n",
      "766 Traning Loss: tensor(38131352.)\n",
      "767 Traning Loss: tensor(38131268.)\n",
      "768 Traning Loss: tensor(38131164.)\n",
      "769 Traning Loss: tensor(38131080.)\n",
      "770 Traning Loss: tensor(38130996.)\n",
      "771 Traning Loss: tensor(38130912.)\n",
      "772 Traning Loss: tensor(38130820.)\n",
      "773 Traning Loss: tensor(38130736.)\n",
      "774 Traning Loss: tensor(38130636.)\n",
      "775 Traning Loss: tensor(38130548.)\n",
      "776 Traning Loss: tensor(38130464.)\n",
      "777 Traning Loss: tensor(38130376.)\n",
      "778 Traning Loss: tensor(38130288.)\n",
      "779 Traning Loss: tensor(38130180.)\n",
      "780 Traning Loss: tensor(38130096.)\n",
      "781 Traning Loss: tensor(38130012.)\n",
      "782 Traning Loss: tensor(38129920.)\n",
      "783 Traning Loss: tensor(38129836.)\n",
      "784 Traning Loss: tensor(38129736.)\n",
      "785 Traning Loss: tensor(38129648.)\n",
      "786 Traning Loss: tensor(38129556.)\n",
      "787 Traning Loss: tensor(38129476.)\n",
      "788 Traning Loss: tensor(38129388.)\n",
      "789 Traning Loss: tensor(38129284.)\n",
      "790 Traning Loss: tensor(38129200.)\n",
      "791 Traning Loss: tensor(38129116.)\n",
      "792 Traning Loss: tensor(38129024.)\n",
      "793 Traning Loss: tensor(38128948.)\n",
      "794 Traning Loss: tensor(38128844.)\n",
      "795 Traning Loss: tensor(38128752.)\n",
      "796 Traning Loss: tensor(38128664.)\n",
      "797 Traning Loss: tensor(38128580.)\n",
      "798 Traning Loss: tensor(38128480.)\n",
      "799 Traning Loss: tensor(38128392.)\n",
      "800 Traning Loss: tensor(38128308.)\n",
      "801 Traning Loss: tensor(38128212.)\n",
      "802 Traning Loss: tensor(38128128.)\n",
      "803 Traning Loss: tensor(38128044.)\n",
      "804 Traning Loss: tensor(38127940.)\n",
      "805 Traning Loss: tensor(38179060.)\n",
      "806 Traning Loss: tensor(38127772.)\n",
      "807 Traning Loss: tensor(38127688.)\n",
      "808 Traning Loss: tensor(38127588.)\n",
      "809 Traning Loss: tensor(38127500.)\n",
      "810 Traning Loss: tensor(38127704.)\n",
      "811 Traning Loss: tensor(38127324.)\n",
      "812 Traning Loss: tensor(38127236.)\n",
      "813 Traning Loss: tensor(38127156.)\n",
      "814 Traning Loss: tensor(38127056.)\n",
      "815 Traning Loss: tensor(38126972.)\n",
      "816 Traning Loss: tensor(38126876.)\n",
      "817 Traning Loss: tensor(38126796.)\n",
      "818 Traning Loss: tensor(38126708.)\n",
      "819 Traning Loss: tensor(38126612.)\n",
      "820 Traning Loss: tensor(38126524.)\n",
      "821 Traning Loss: tensor(38126436.)\n",
      "822 Traning Loss: tensor(38126352.)\n",
      "823 Traning Loss: tensor(38126268.)\n",
      "824 Traning Loss: tensor(38126164.)\n",
      "825 Traning Loss: tensor(38126080.)\n",
      "826 Traning Loss: tensor(38125996.)\n",
      "827 Traning Loss: tensor(38125900.)\n",
      "828 Traning Loss: tensor(38125816.)\n",
      "829 Traning Loss: tensor(38125720.)\n",
      "830 Traning Loss: tensor(38177032.)\n",
      "831 Traning Loss: tensor(38125544.)\n",
      "832 Traning Loss: tensor(38125460.)\n",
      "833 Traning Loss: tensor(38125368.)\n",
      "834 Traning Loss: tensor(38125292.)\n",
      "835 Traning Loss: tensor(38125184.)\n",
      "836 Traning Loss: tensor(38125100.)\n",
      "837 Traning Loss: tensor(38125020.)\n",
      "838 Traning Loss: tensor(38124932.)\n",
      "839 Traning Loss: tensor(38124832.)\n",
      "840 Traning Loss: tensor(38124748.)\n",
      "841 Traning Loss: tensor(38124660.)\n",
      "842 Traning Loss: tensor(38124576.)\n",
      "843 Traning Loss: tensor(38124480.)\n",
      "844 Traning Loss: tensor(38124396.)\n",
      "845 Traning Loss: tensor(38124292.)\n",
      "846 Traning Loss: tensor(38124204.)\n",
      "847 Traning Loss: tensor(38124124.)\n",
      "848 Traning Loss: tensor(38124036.)\n",
      "849 Traning Loss: tensor(38123936.)\n",
      "850 Traning Loss: tensor(38123852.)\n",
      "851 Traning Loss: tensor(38123764.)\n",
      "852 Traning Loss: tensor(38123676.)\n",
      "853 Traning Loss: tensor(38123588.)\n",
      "854 Traning Loss: tensor(38123488.)\n",
      "855 Traning Loss: tensor(38125992.)\n",
      "856 Traning Loss: tensor(38123328.)\n",
      "857 Traning Loss: tensor(38123248.)\n",
      "858 Traning Loss: tensor(38123160.)\n",
      "859 Traning Loss: tensor(38123076.)\n",
      "860 Traning Loss: tensor(38122988.)\n",
      "861 Traning Loss: tensor(38122912.)\n",
      "862 Traning Loss: tensor(38122824.)\n",
      "863 Traning Loss: tensor(38122748.)\n",
      "864 Traning Loss: tensor(38122660.)\n",
      "865 Traning Loss: tensor(38122576.)\n",
      "866 Traning Loss: tensor(38122492.)\n",
      "867 Traning Loss: tensor(38122412.)\n",
      "868 Traning Loss: tensor(38122328.)\n",
      "869 Traning Loss: tensor(38122236.)\n",
      "870 Traning Loss: tensor(38122152.)\n",
      "871 Traning Loss: tensor(38122072.)\n",
      "872 Traning Loss: tensor(38121984.)\n",
      "873 Traning Loss: tensor(38121896.)\n",
      "874 Traning Loss: tensor(38121820.)\n",
      "875 Traning Loss: tensor(38121716.)\n",
      "876 Traning Loss: tensor(38121636.)\n",
      "877 Traning Loss: tensor(38121540.)\n",
      "878 Traning Loss: tensor(38121440.)\n",
      "879 Traning Loss: tensor(38121352.)\n",
      "880 Traning Loss: tensor(38121268.)\n",
      "881 Traning Loss: tensor(38121184.)\n",
      "882 Traning Loss: tensor(38121096.)\n",
      "883 Traning Loss: tensor(38121008.)\n",
      "884 Traning Loss: tensor(38141216.)\n",
      "885 Traning Loss: tensor(38120876.)\n",
      "886 Traning Loss: tensor(38120880.)\n",
      "887 Traning Loss: tensor(38120896.)\n",
      "888 Traning Loss: tensor(38170344.)\n",
      "889 Traning Loss: tensor(38121056.)\n",
      "890 Traning Loss: tensor(38121212.)\n",
      "891 Traning Loss: tensor(38121392.)\n",
      "892 Traning Loss: tensor(38121556.)\n",
      "893 Traning Loss: tensor(38121712.)\n",
      "894 Traning Loss: tensor(38121788.)\n",
      "895 Traning Loss: tensor(38121764.)\n",
      "896 Traning Loss: tensor(38121664.)\n",
      "897 Traning Loss: tensor(38121476.)\n",
      "898 Traning Loss: tensor(38121240.)\n",
      "899 Traning Loss: tensor(38136360.)\n",
      "900 Traning Loss: tensor(38120836.)\n",
      "901 Traning Loss: tensor(38120684.)\n",
      "902 Traning Loss: tensor(38120492.)\n",
      "903 Traning Loss: tensor(38120288.)\n",
      "904 Traning Loss: tensor(38120080.)\n",
      "905 Traning Loss: tensor(38119856.)\n",
      "906 Traning Loss: tensor(38119656.)\n",
      "907 Traning Loss: tensor(38119472.)\n",
      "908 Traning Loss: tensor(38119296.)\n",
      "909 Traning Loss: tensor(38119136.)\n",
      "910 Traning Loss: tensor(38118960.)\n",
      "911 Traning Loss: tensor(38118816.)\n",
      "912 Traning Loss: tensor(38118688.)\n",
      "913 Traning Loss: tensor(38118556.)\n",
      "914 Traning Loss: tensor(38118436.)\n",
      "915 Traning Loss: tensor(38118308.)\n",
      "916 Traning Loss: tensor(38118196.)\n",
      "917 Traning Loss: tensor(38118084.)\n",
      "918 Traning Loss: tensor(38117980.)\n",
      "919 Traning Loss: tensor(38117864.)\n",
      "920 Traning Loss: tensor(38117768.)\n",
      "921 Traning Loss: tensor(38117668.)\n",
      "922 Traning Loss: tensor(38117572.)\n",
      "923 Traning Loss: tensor(38117476.)\n",
      "924 Traning Loss: tensor(38117360.)\n",
      "925 Traning Loss: tensor(38117268.)\n",
      "926 Traning Loss: tensor(38117180.)\n",
      "927 Traning Loss: tensor(38117084.)\n",
      "928 Traning Loss: tensor(38116992.)\n",
      "929 Traning Loss: tensor(38116888.)\n",
      "930 Traning Loss: tensor(38116800.)\n",
      "931 Traning Loss: tensor(38116708.)\n",
      "932 Traning Loss: tensor(38116620.)\n",
      "933 Traning Loss: tensor(38116536.)\n",
      "934 Traning Loss: tensor(38116428.)\n",
      "935 Traning Loss: tensor(38144860.)\n",
      "936 Traning Loss: tensor(38116288.)\n",
      "937 Traning Loss: tensor(38116236.)\n",
      "938 Traning Loss: tensor(38116204.)\n",
      "939 Traning Loss: tensor(38116164.)\n",
      "940 Traning Loss: tensor(38116128.)\n",
      "941 Traning Loss: tensor(38116124.)\n",
      "942 Traning Loss: tensor(38116092.)\n",
      "943 Traning Loss: tensor(38116080.)\n",
      "944 Traning Loss: tensor(38116064.)\n",
      "945 Traning Loss: tensor(38116024.)\n",
      "946 Traning Loss: tensor(38115988.)\n",
      "947 Traning Loss: tensor(38115940.)\n",
      "948 Traning Loss: tensor(38115880.)\n",
      "949 Traning Loss: tensor(38115808.)\n",
      "950 Traning Loss: tensor(38115700.)\n",
      "951 Traning Loss: tensor(38115604.)\n",
      "952 Traning Loss: tensor(38115496.)\n",
      "953 Traning Loss: tensor(38115380.)\n",
      "954 Traning Loss: tensor(38115260.)\n",
      "955 Traning Loss: tensor(38115108.)\n",
      "956 Traning Loss: tensor(38114992.)\n",
      "957 Traning Loss: tensor(38114856.)\n",
      "958 Traning Loss: tensor(38114732.)\n",
      "959 Traning Loss: tensor(38114592.)\n",
      "960 Traning Loss: tensor(38114472.)\n",
      "961 Traning Loss: tensor(38114352.)\n",
      "962 Traning Loss: tensor(38114236.)\n",
      "963 Traning Loss: tensor(38114124.)\n",
      "964 Traning Loss: tensor(38113992.)\n",
      "965 Traning Loss: tensor(38113884.)\n",
      "966 Traning Loss: tensor(38113780.)\n",
      "967 Traning Loss: tensor(38113676.)\n",
      "968 Traning Loss: tensor(38113576.)\n",
      "969 Traning Loss: tensor(38113456.)\n",
      "970 Traning Loss: tensor(38113360.)\n",
      "971 Traning Loss: tensor(38113252.)\n",
      "972 Traning Loss: tensor(38113152.)\n",
      "973 Traning Loss: tensor(38113068.)\n",
      "974 Traning Loss: tensor(38112964.)\n",
      "975 Traning Loss: tensor(38112856.)\n",
      "976 Traning Loss: tensor(38164304.)\n",
      "977 Traning Loss: tensor(38112680.)\n",
      "978 Traning Loss: tensor(38112592.)\n",
      "979 Traning Loss: tensor(38112480.)\n",
      "980 Traning Loss: tensor(38112396.)\n",
      "981 Traning Loss: tensor(38112308.)\n",
      "982 Traning Loss: tensor(38112228.)\n",
      "983 Traning Loss: tensor(38112132.)\n",
      "984 Traning Loss: tensor(38112044.)\n",
      "985 Traning Loss: tensor(38111944.)\n",
      "986 Traning Loss: tensor(38111856.)\n",
      "987 Traning Loss: tensor(38111756.)\n",
      "988 Traning Loss: tensor(38111676.)\n",
      "989 Traning Loss: tensor(38111580.)\n",
      "990 Traning Loss: tensor(38111488.)\n",
      "991 Traning Loss: tensor(38111392.)\n",
      "992 Traning Loss: tensor(38111300.)\n",
      "993 Traning Loss: tensor(38111212.)\n",
      "994 Traning Loss: tensor(38111128.)\n",
      "995 Traning Loss: tensor(38111020.)\n",
      "996 Traning Loss: tensor(38110936.)\n",
      "997 Traning Loss: tensor(38110848.)\n",
      "998 Traning Loss: tensor(38110764.)\n",
      "999 Traning Loss: tensor(38110660.)\n",
      "1000 Traning Loss: tensor(38110580.)\n",
      "1001 Traning Loss: tensor(38110480.)\n",
      "1002 Traning Loss: tensor(38110396.)\n",
      "1003 Traning Loss: tensor(38110308.)\n",
      "1004 Traning Loss: tensor(38110216.)\n",
      "1005 Traning Loss: tensor(38110116.)\n",
      "1006 Traning Loss: tensor(38110036.)\n",
      "1007 Traning Loss: tensor(38109948.)\n",
      "1008 Traning Loss: tensor(38109852.)\n",
      "1009 Traning Loss: tensor(38109752.)\n",
      "1010 Traning Loss: tensor(38109664.)\n",
      "1011 Traning Loss: tensor(38109576.)\n",
      "1012 Traning Loss: tensor(38109488.)\n",
      "1013 Traning Loss: tensor(38109404.)\n",
      "1014 Traning Loss: tensor(38109316.)\n",
      "1015 Traning Loss: tensor(38109216.)\n",
      "1016 Traning Loss: tensor(38109128.)\n",
      "1017 Traning Loss: tensor(38109040.)\n",
      "1018 Traning Loss: tensor(38108952.)\n",
      "1019 Traning Loss: tensor(38108852.)\n",
      "1020 Traning Loss: tensor(38108756.)\n",
      "1021 Traning Loss: tensor(38108680.)\n",
      "1022 Traning Loss: tensor(38108592.)\n",
      "1023 Traning Loss: tensor(38108496.)\n",
      "1024 Traning Loss: tensor(38108412.)\n",
      "1025 Traning Loss: tensor(38108308.)\n",
      "1026 Traning Loss: tensor(38108220.)\n",
      "1027 Traning Loss: tensor(38108132.)\n",
      "1028 Traning Loss: tensor(38108048.)\n",
      "1029 Traning Loss: tensor(38107952.)\n",
      "1030 Traning Loss: tensor(38107860.)\n",
      "1031 Traning Loss: tensor(38107776.)\n",
      "1032 Traning Loss: tensor(38107688.)\n",
      "1033 Traning Loss: tensor(38107596.)\n",
      "1034 Traning Loss: tensor(38107520.)\n",
      "1035 Traning Loss: tensor(38107416.)\n",
      "1036 Traning Loss: tensor(38107332.)\n",
      "1037 Traning Loss: tensor(38107232.)\n",
      "1038 Traning Loss: tensor(38107148.)\n",
      "1039 Traning Loss: tensor(38107044.)\n",
      "1040 Traning Loss: tensor(38106964.)\n",
      "1041 Traning Loss: tensor(38106876.)\n",
      "1042 Traning Loss: tensor(38106788.)\n",
      "1043 Traning Loss: tensor(38106708.)\n",
      "1044 Traning Loss: tensor(38106604.)\n",
      "1045 Traning Loss: tensor(38106516.)\n",
      "1046 Traning Loss: tensor(38106432.)\n",
      "1047 Traning Loss: tensor(38106344.)\n",
      "1048 Traning Loss: tensor(38106256.)\n",
      "1049 Traning Loss: tensor(38106152.)\n",
      "1050 Traning Loss: tensor(38106068.)\n",
      "1051 Traning Loss: tensor(38105984.)\n",
      "1052 Traning Loss: tensor(38105888.)\n",
      "1053 Traning Loss: tensor(38105812.)\n",
      "1054 Traning Loss: tensor(38105716.)\n",
      "1055 Traning Loss: tensor(38105620.)\n",
      "1056 Traning Loss: tensor(38105528.)\n",
      "1057 Traning Loss: tensor(38105444.)\n",
      "1058 Traning Loss: tensor(38158092.)\n",
      "1059 Traning Loss: tensor(38105260.)\n",
      "1060 Traning Loss: tensor(38105176.)\n",
      "1061 Traning Loss: tensor(38105092.)\n",
      "1062 Traning Loss: tensor(38105008.)\n",
      "1063 Traning Loss: tensor(38104920.)\n",
      "1064 Traning Loss: tensor(38104840.)\n",
      "1065 Traning Loss: tensor(38104736.)\n",
      "1066 Traning Loss: tensor(38104652.)\n",
      "1067 Traning Loss: tensor(38104564.)\n",
      "1068 Traning Loss: tensor(38104480.)\n",
      "1069 Traning Loss: tensor(38104396.)\n",
      "1070 Traning Loss: tensor(38104288.)\n",
      "1071 Traning Loss: tensor(38104204.)\n",
      "1072 Traning Loss: tensor(38104112.)\n",
      "1073 Traning Loss: tensor(38104024.)\n",
      "1074 Traning Loss: tensor(38103948.)\n",
      "1075 Traning Loss: tensor(38103844.)\n",
      "1076 Traning Loss: tensor(38103760.)\n",
      "1077 Traning Loss: tensor(38103672.)\n",
      "1078 Traning Loss: tensor(38103584.)\n",
      "1079 Traning Loss: tensor(38103484.)\n",
      "1080 Traning Loss: tensor(38103400.)\n",
      "1081 Traning Loss: tensor(38103312.)\n",
      "1082 Traning Loss: tensor(38103228.)\n",
      "1083 Traning Loss: tensor(38103140.)\n",
      "1084 Traning Loss: tensor(38103056.)\n",
      "1085 Traning Loss: tensor(38102948.)\n",
      "1086 Traning Loss: tensor(38102864.)\n",
      "1087 Traning Loss: tensor(38102780.)\n",
      "1088 Traning Loss: tensor(38102688.)\n",
      "1089 Traning Loss: tensor(38102592.)\n",
      "1090 Traning Loss: tensor(38102504.)\n",
      "1091 Traning Loss: tensor(38102420.)\n",
      "1092 Traning Loss: tensor(38102328.)\n",
      "1093 Traning Loss: tensor(38102244.)\n",
      "1094 Traning Loss: tensor(38102156.)\n",
      "1095 Traning Loss: tensor(38102052.)\n",
      "1096 Traning Loss: tensor(38101976.)\n",
      "1097 Traning Loss: tensor(38101888.)\n",
      "1098 Traning Loss: tensor(38101800.)\n",
      "1099 Traning Loss: tensor(38101692.)\n",
      "1100 Traning Loss: tensor(38101604.)\n",
      "1101 Traning Loss: tensor(38101528.)\n",
      "1102 Traning Loss: tensor(38101432.)\n",
      "1103 Traning Loss: tensor(38101344.)\n",
      "1104 Traning Loss: tensor(38101264.)\n",
      "1105 Traning Loss: tensor(38101164.)\n",
      "1106 Traning Loss: tensor(38101076.)\n",
      "1107 Traning Loss: tensor(38100988.)\n",
      "1108 Traning Loss: tensor(38100904.)\n",
      "1109 Traning Loss: tensor(38100816.)\n",
      "1110 Traning Loss: tensor(38100712.)\n",
      "1111 Traning Loss: tensor(38100628.)\n",
      "1112 Traning Loss: tensor(38100544.)\n",
      "1113 Traning Loss: tensor(38100460.)\n",
      "1114 Traning Loss: tensor(38100376.)\n",
      "1115 Traning Loss: tensor(38100264.)\n",
      "1116 Traning Loss: tensor(38100188.)\n",
      "1117 Traning Loss: tensor(38106164.)\n",
      "1118 Traning Loss: tensor(38100020.)\n",
      "1119 Traning Loss: tensor(38099932.)\n",
      "1120 Traning Loss: tensor(38099876.)\n",
      "1121 Traning Loss: tensor(38099800.)\n",
      "1122 Traning Loss: tensor(38099732.)\n",
      "1123 Traning Loss: tensor(38099676.)\n",
      "1124 Traning Loss: tensor(38099592.)\n",
      "1125 Traning Loss: tensor(38099536.)\n",
      "1126 Traning Loss: tensor(38099476.)\n",
      "1127 Traning Loss: tensor(38099420.)\n",
      "1128 Traning Loss: tensor(38099356.)\n",
      "1129 Traning Loss: tensor(38099272.)\n",
      "1130 Traning Loss: tensor(38099208.)\n",
      "1131 Traning Loss: tensor(38099128.)\n",
      "1132 Traning Loss: tensor(38099056.)\n",
      "1133 Traning Loss: tensor(38098992.)\n",
      "1134 Traning Loss: tensor(38098904.)\n",
      "1135 Traning Loss: tensor(38098808.)\n",
      "1136 Traning Loss: tensor(38098724.)\n",
      "1137 Traning Loss: tensor(38098636.)\n",
      "1138 Traning Loss: tensor(38098548.)\n",
      "1139 Traning Loss: tensor(38098444.)\n",
      "1140 Traning Loss: tensor(38098352.)\n",
      "1141 Traning Loss: tensor(38098260.)\n",
      "1142 Traning Loss: tensor(38098160.)\n",
      "1143 Traning Loss: tensor(38098064.)\n",
      "1144 Traning Loss: tensor(38097968.)\n",
      "1145 Traning Loss: tensor(38097860.)\n",
      "1146 Traning Loss: tensor(38097752.)\n",
      "1147 Traning Loss: tensor(38097652.)\n",
      "1148 Traning Loss: tensor(38097568.)\n",
      "1149 Traning Loss: tensor(38097456.)\n",
      "1150 Traning Loss: tensor(38097356.)\n",
      "1151 Traning Loss: tensor(38097260.)\n",
      "1152 Traning Loss: tensor(38097172.)\n",
      "1153 Traning Loss: tensor(38097064.)\n",
      "1154 Traning Loss: tensor(38151484.)\n",
      "1155 Traning Loss: tensor(38096868.)\n",
      "1156 Traning Loss: tensor(38096776.)\n",
      "1157 Traning Loss: tensor(38096680.)\n",
      "1158 Traning Loss: tensor(38096592.)\n",
      "1159 Traning Loss: tensor(38096484.)\n",
      "1160 Traning Loss: tensor(38096400.)\n",
      "1161 Traning Loss: tensor(38096300.)\n",
      "1162 Traning Loss: tensor(38096212.)\n",
      "1163 Traning Loss: tensor(38096120.)\n",
      "1164 Traning Loss: tensor(38096028.)\n",
      "1165 Traning Loss: tensor(38095916.)\n",
      "1166 Traning Loss: tensor(38095840.)\n",
      "1167 Traning Loss: tensor(38095752.)\n",
      "1168 Traning Loss: tensor(38095652.)\n",
      "1169 Traning Loss: tensor(38096440.)\n",
      "1170 Traning Loss: tensor(38095464.)\n",
      "1171 Traning Loss: tensor(38095380.)\n",
      "1172 Traning Loss: tensor(38095288.)\n",
      "1173 Traning Loss: tensor(38095204.)\n",
      "1174 Traning Loss: tensor(38095120.)\n",
      "1175 Traning Loss: tensor(38095020.)\n",
      "1176 Traning Loss: tensor(38094932.)\n",
      "1177 Traning Loss: tensor(38094844.)\n",
      "1178 Traning Loss: tensor(38094756.)\n",
      "1179 Traning Loss: tensor(38094672.)\n",
      "1180 Traning Loss: tensor(38094560.)\n",
      "1181 Traning Loss: tensor(38094484.)\n",
      "1182 Traning Loss: tensor(38094396.)\n",
      "1183 Traning Loss: tensor(38094300.)\n",
      "1184 Traning Loss: tensor(38094212.)\n",
      "1185 Traning Loss: tensor(38094112.)\n",
      "1186 Traning Loss: tensor(38094028.)\n",
      "1187 Traning Loss: tensor(38093932.)\n",
      "1188 Traning Loss: tensor(38093848.)\n",
      "1189 Traning Loss: tensor(38093764.)\n",
      "1190 Traning Loss: tensor(38093656.)\n",
      "1191 Traning Loss: tensor(38093568.)\n",
      "1192 Traning Loss: tensor(38093484.)\n",
      "1193 Traning Loss: tensor(38093396.)\n",
      "1194 Traning Loss: tensor(38093312.)\n",
      "1195 Traning Loss: tensor(38093208.)\n",
      "1196 Traning Loss: tensor(38093120.)\n",
      "1197 Traning Loss: tensor(38093020.)\n",
      "1198 Traning Loss: tensor(38092944.)\n",
      "1199 Traning Loss: tensor(38092832.)\n",
      "1200 Traning Loss: tensor(38092748.)\n",
      "1201 Traning Loss: tensor(38092668.)\n",
      "1202 Traning Loss: tensor(38092572.)\n",
      "1203 Traning Loss: tensor(38092488.)\n",
      "1204 Traning Loss: tensor(38092400.)\n",
      "1205 Traning Loss: tensor(38092296.)\n",
      "1206 Traning Loss: tensor(38092212.)\n",
      "1207 Traning Loss: tensor(38092124.)\n",
      "1208 Traning Loss: tensor(38092036.)\n",
      "1209 Traning Loss: tensor(38091936.)\n",
      "1210 Traning Loss: tensor(38091852.)\n",
      "1211 Traning Loss: tensor(38091768.)\n",
      "1212 Traning Loss: tensor(38091668.)\n",
      "1213 Traning Loss: tensor(38091592.)\n",
      "1214 Traning Loss: tensor(38091504.)\n",
      "1215 Traning Loss: tensor(38091396.)\n",
      "1216 Traning Loss: tensor(38091308.)\n",
      "1217 Traning Loss: tensor(38091220.)\n",
      "1218 Traning Loss: tensor(38091136.)\n",
      "1219 Traning Loss: tensor(38091032.)\n",
      "1220 Traning Loss: tensor(38090948.)\n",
      "1221 Traning Loss: tensor(38090860.)\n",
      "1222 Traning Loss: tensor(38090772.)\n",
      "1223 Traning Loss: tensor(38090688.)\n",
      "1224 Traning Loss: tensor(38090600.)\n",
      "1225 Traning Loss: tensor(38090500.)\n",
      "1226 Traning Loss: tensor(38090408.)\n",
      "1227 Traning Loss: tensor(38090324.)\n",
      "1228 Traning Loss: tensor(38090240.)\n",
      "1229 Traning Loss: tensor(38090128.)\n",
      "1230 Traning Loss: tensor(38090096.)\n",
      "1231 Traning Loss: tensor(38089964.)\n",
      "1232 Traning Loss: tensor(38089880.)\n",
      "1233 Traning Loss: tensor(38089788.)\n",
      "1234 Traning Loss: tensor(38089704.)\n",
      "1235 Traning Loss: tensor(38089604.)\n",
      "1236 Traning Loss: tensor(38089512.)\n",
      "1237 Traning Loss: tensor(38089428.)\n",
      "1238 Traning Loss: tensor(38089344.)\n",
      "1239 Traning Loss: tensor(38089256.)\n",
      "1240 Traning Loss: tensor(38089156.)\n",
      "1241 Traning Loss: tensor(38089068.)\n",
      "1242 Traning Loss: tensor(38088980.)\n",
      "1243 Traning Loss: tensor(38088896.)\n",
      "1244 Traning Loss: tensor(38088812.)\n",
      "1245 Traning Loss: tensor(38088700.)\n",
      "1246 Traning Loss: tensor(38088616.)\n",
      "1247 Traning Loss: tensor(38088528.)\n",
      "1248 Traning Loss: tensor(38088444.)\n",
      "1249 Traning Loss: tensor(38088344.)\n",
      "1250 Traning Loss: tensor(38088256.)\n",
      "1251 Traning Loss: tensor(38088172.)\n",
      "1252 Traning Loss: tensor(38088084.)\n",
      "1253 Traning Loss: tensor(38088004.)\n",
      "1254 Traning Loss: tensor(38087900.)\n",
      "1255 Traning Loss: tensor(38087812.)\n",
      "1256 Traning Loss: tensor(38087728.)\n",
      "1257 Traning Loss: tensor(38087648.)\n",
      "1258 Traning Loss: tensor(38087556.)\n",
      "1259 Traning Loss: tensor(38087456.)\n",
      "1260 Traning Loss: tensor(38087368.)\n",
      "1261 Traning Loss: tensor(38087272.)\n",
      "1262 Traning Loss: tensor(38087184.)\n",
      "1263 Traning Loss: tensor(38087100.)\n",
      "1264 Traning Loss: tensor(38087016.)\n",
      "1265 Traning Loss: tensor(38086912.)\n",
      "1266 Traning Loss: tensor(38086832.)\n",
      "1267 Traning Loss: tensor(38086740.)\n",
      "1268 Traning Loss: tensor(38086656.)\n",
      "1269 Traning Loss: tensor(38086556.)\n",
      "1270 Traning Loss: tensor(38086468.)\n",
      "1271 Traning Loss: tensor(38086388.)\n",
      "1272 Traning Loss: tensor(38086288.)\n",
      "1273 Traning Loss: tensor(38086212.)\n",
      "1274 Traning Loss: tensor(38086132.)\n",
      "1275 Traning Loss: tensor(38086028.)\n",
      "1276 Traning Loss: tensor(38085936.)\n",
      "1277 Traning Loss: tensor(38085844.)\n",
      "1278 Traning Loss: tensor(38085760.)\n",
      "1279 Traning Loss: tensor(38085660.)\n",
      "1280 Traning Loss: tensor(38085576.)\n",
      "1281 Traning Loss: tensor(38085492.)\n",
      "1282 Traning Loss: tensor(38085400.)\n",
      "1283 Traning Loss: tensor(38085316.)\n",
      "1284 Traning Loss: tensor(38085228.)\n",
      "1285 Traning Loss: tensor(38085132.)\n",
      "1286 Traning Loss: tensor(38085044.)\n",
      "1287 Traning Loss: tensor(38084960.)\n",
      "1288 Traning Loss: tensor(38084876.)\n",
      "1289 Traning Loss: tensor(38084772.)\n",
      "1290 Traning Loss: tensor(38084688.)\n",
      "1291 Traning Loss: tensor(38084604.)\n",
      "1292 Traning Loss: tensor(38084504.)\n",
      "1293 Traning Loss: tensor(38084420.)\n",
      "1294 Traning Loss: tensor(38084316.)\n",
      "1295 Traning Loss: tensor(38084236.)\n",
      "1296 Traning Loss: tensor(38084148.)\n",
      "1297 Traning Loss: tensor(38084060.)\n",
      "1298 Traning Loss: tensor(38083972.)\n",
      "1299 Traning Loss: tensor(38083876.)\n",
      "1300 Traning Loss: tensor(38083788.)\n",
      "1301 Traning Loss: tensor(38083700.)\n",
      "1302 Traning Loss: tensor(38083612.)\n",
      "1303 Traning Loss: tensor(38083532.)\n",
      "1304 Traning Loss: tensor(38083428.)\n",
      "1305 Traning Loss: tensor(38083348.)\n",
      "1306 Traning Loss: tensor(38083264.)\n",
      "1307 Traning Loss: tensor(38083180.)\n",
      "1308 Traning Loss: tensor(38083084.)\n",
      "1309 Traning Loss: tensor(38082988.)\n",
      "1310 Traning Loss: tensor(38082904.)\n",
      "1311 Traning Loss: tensor(38082808.)\n",
      "1312 Traning Loss: tensor(38082724.)\n",
      "1313 Traning Loss: tensor(38082636.)\n",
      "1314 Traning Loss: tensor(38082556.)\n",
      "1315 Traning Loss: tensor(38082452.)\n",
      "1316 Traning Loss: tensor(38082364.)\n",
      "1317 Traning Loss: tensor(38082276.)\n",
      "1318 Traning Loss: tensor(38082192.)\n",
      "1319 Traning Loss: tensor(38082092.)\n",
      "1320 Traning Loss: tensor(38082008.)\n",
      "1321 Traning Loss: tensor(38081920.)\n",
      "1322 Traning Loss: tensor(38081832.)\n",
      "1323 Traning Loss: tensor(38081736.)\n",
      "1324 Traning Loss: tensor(38081656.)\n",
      "1325 Traning Loss: tensor(38081556.)\n",
      "1326 Traning Loss: tensor(38081468.)\n",
      "1327 Traning Loss: tensor(38138048.)\n",
      "1328 Traning Loss: tensor(38081296.)\n",
      "1329 Traning Loss: tensor(38081196.)\n",
      "1330 Traning Loss: tensor(38081108.)\n",
      "1331 Traning Loss: tensor(38081032.)\n",
      "1332 Traning Loss: tensor(38080944.)\n",
      "1333 Traning Loss: tensor(38080856.)\n",
      "1334 Traning Loss: tensor(38080772.)\n",
      "1335 Traning Loss: tensor(38080668.)\n",
      "1336 Traning Loss: tensor(38080584.)\n",
      "1337 Traning Loss: tensor(38080508.)\n",
      "1338 Traning Loss: tensor(38080404.)\n",
      "1339 Traning Loss: tensor(38080316.)\n",
      "1340 Traning Loss: tensor(38080228.)\n",
      "1341 Traning Loss: tensor(38080144.)\n",
      "1342 Traning Loss: tensor(38080048.)\n",
      "1343 Traning Loss: tensor(38079964.)\n",
      "1344 Traning Loss: tensor(38079876.)\n",
      "1345 Traning Loss: tensor(38079780.)\n",
      "1346 Traning Loss: tensor(38079696.)\n",
      "1347 Traning Loss: tensor(38079604.)\n",
      "1348 Traning Loss: tensor(38079520.)\n",
      "1349 Traning Loss: tensor(38079420.)\n",
      "1350 Traning Loss: tensor(38079336.)\n",
      "1351 Traning Loss: tensor(38079252.)\n",
      "1352 Traning Loss: tensor(38079160.)\n",
      "1353 Traning Loss: tensor(38079076.)\n",
      "1354 Traning Loss: tensor(38078992.)\n",
      "1355 Traning Loss: tensor(38078888.)\n",
      "1356 Traning Loss: tensor(38078804.)\n",
      "1357 Traning Loss: tensor(38078712.)\n",
      "1358 Traning Loss: tensor(38078636.)\n",
      "1359 Traning Loss: tensor(38135412.)\n",
      "1360 Traning Loss: tensor(38078444.)\n",
      "1361 Traning Loss: tensor(38078360.)\n",
      "1362 Traning Loss: tensor(38078272.)\n",
      "1363 Traning Loss: tensor(38078188.)\n",
      "1364 Traning Loss: tensor(38078100.)\n",
      "1365 Traning Loss: tensor(38078000.)\n",
      "1366 Traning Loss: tensor(38077920.)\n",
      "1367 Traning Loss: tensor(38077836.)\n",
      "1368 Traning Loss: tensor(38077744.)\n",
      "1369 Traning Loss: tensor(38077656.)\n",
      "1370 Traning Loss: tensor(38077556.)\n",
      "1371 Traning Loss: tensor(38077476.)\n",
      "1372 Traning Loss: tensor(38077384.)\n",
      "1373 Traning Loss: tensor(38077300.)\n",
      "1374 Traning Loss: tensor(38077216.)\n",
      "1375 Traning Loss: tensor(38077132.)\n",
      "1376 Traning Loss: tensor(38077020.)\n",
      "1377 Traning Loss: tensor(38076936.)\n",
      "1378 Traning Loss: tensor(38076852.)\n",
      "1379 Traning Loss: tensor(38076764.)\n",
      "1380 Traning Loss: tensor(38076668.)\n",
      "1381 Traning Loss: tensor(38076580.)\n",
      "1382 Traning Loss: tensor(38076500.)\n",
      "1383 Traning Loss: tensor(38076408.)\n",
      "1384 Traning Loss: tensor(38076320.)\n",
      "1385 Traning Loss: tensor(38076236.)\n",
      "1386 Traning Loss: tensor(38076132.)\n",
      "1387 Traning Loss: tensor(38076052.)\n",
      "1388 Traning Loss: tensor(38075964.)\n",
      "1389 Traning Loss: tensor(38075876.)\n",
      "1390 Traning Loss: tensor(38075768.)\n",
      "1391 Traning Loss: tensor(38075688.)\n",
      "1392 Traning Loss: tensor(38075612.)\n",
      "1393 Traning Loss: tensor(38075512.)\n",
      "1394 Traning Loss: tensor(38075428.)\n",
      "1395 Traning Loss: tensor(38075348.)\n",
      "1396 Traning Loss: tensor(38075244.)\n",
      "1397 Traning Loss: tensor(38075156.)\n",
      "1398 Traning Loss: tensor(38075068.)\n",
      "1399 Traning Loss: tensor(38074980.)\n",
      "1400 Traning Loss: tensor(38074880.)\n",
      "1401 Traning Loss: tensor(38074800.)\n",
      "1402 Traning Loss: tensor(38074716.)\n",
      "1403 Traning Loss: tensor(38074624.)\n",
      "1404 Traning Loss: tensor(38074540.)\n",
      "1405 Traning Loss: tensor(38074436.)\n",
      "1406 Traning Loss: tensor(38074344.)\n",
      "1407 Traning Loss: tensor(38074272.)\n",
      "1408 Traning Loss: tensor(38074176.)\n",
      "1409 Traning Loss: tensor(38074092.)\n",
      "1410 Traning Loss: tensor(38073988.)\n",
      "1411 Traning Loss: tensor(38073904.)\n",
      "1412 Traning Loss: tensor(38073816.)\n",
      "1413 Traning Loss: tensor(38073736.)\n",
      "1414 Traning Loss: tensor(38073652.)\n",
      "1415 Traning Loss: tensor(38081512.)\n",
      "1416 Traning Loss: tensor(38073472.)\n",
      "1417 Traning Loss: tensor(38073400.)\n",
      "1418 Traning Loss: tensor(38073320.)\n",
      "1419 Traning Loss: tensor(38073256.)\n",
      "1420 Traning Loss: tensor(38073172.)\n",
      "1421 Traning Loss: tensor(38073112.)\n",
      "1422 Traning Loss: tensor(38073040.)\n",
      "1423 Traning Loss: tensor(38072980.)\n",
      "1424 Traning Loss: tensor(38072924.)\n",
      "1425 Traning Loss: tensor(38072868.)\n",
      "1426 Traning Loss: tensor(38072800.)\n",
      "1427 Traning Loss: tensor(38072736.)\n",
      "1428 Traning Loss: tensor(38072680.)\n",
      "1429 Traning Loss: tensor(38072620.)\n",
      "1430 Traning Loss: tensor(38128732.)\n",
      "1431 Traning Loss: tensor(38072480.)\n",
      "1432 Traning Loss: tensor(38072424.)\n",
      "1433 Traning Loss: tensor(38072364.)\n",
      "1434 Traning Loss: tensor(38072292.)\n",
      "1435 Traning Loss: tensor(38072220.)\n",
      "1436 Traning Loss: tensor(38072128.)\n",
      "1437 Traning Loss: tensor(38072044.)\n",
      "1438 Traning Loss: tensor(38071944.)\n",
      "1439 Traning Loss: tensor(38071864.)\n",
      "1440 Traning Loss: tensor(38071756.)\n",
      "1441 Traning Loss: tensor(38071664.)\n",
      "1442 Traning Loss: tensor(38071552.)\n",
      "1443 Traning Loss: tensor(38071452.)\n",
      "1444 Traning Loss: tensor(38071348.)\n",
      "1445 Traning Loss: tensor(38071248.)\n",
      "1446 Traning Loss: tensor(38071128.)\n",
      "1447 Traning Loss: tensor(38071024.)\n",
      "1448 Traning Loss: tensor(38070920.)\n",
      "1449 Traning Loss: tensor(38070816.)\n",
      "1450 Traning Loss: tensor(38070700.)\n",
      "1451 Traning Loss: tensor(38070604.)\n",
      "1452 Traning Loss: tensor(38070500.)\n",
      "1453 Traning Loss: tensor(38070404.)\n",
      "1454 Traning Loss: tensor(38070304.)\n",
      "1455 Traning Loss: tensor(38070200.)\n",
      "1456 Traning Loss: tensor(38070092.)\n",
      "1457 Traning Loss: tensor(38070004.)\n",
      "1458 Traning Loss: tensor(38069908.)\n",
      "1459 Traning Loss: tensor(38127728.)\n",
      "1460 Traning Loss: tensor(38069712.)\n",
      "1461 Traning Loss: tensor(38069604.)\n",
      "1462 Traning Loss: tensor(38069520.)\n",
      "1463 Traning Loss: tensor(38069424.)\n",
      "1464 Traning Loss: tensor(38069336.)\n",
      "1465 Traning Loss: tensor(38069244.)\n",
      "1466 Traning Loss: tensor(38069140.)\n",
      "1467 Traning Loss: tensor(38069048.)\n",
      "1468 Traning Loss: tensor(38068960.)\n",
      "1469 Traning Loss: tensor(38068864.)\n",
      "1470 Traning Loss: tensor(38068780.)\n",
      "1471 Traning Loss: tensor(38068672.)\n",
      "1472 Traning Loss: tensor(38068592.)\n",
      "1473 Traning Loss: tensor(38068500.)\n",
      "1474 Traning Loss: tensor(38068404.)\n",
      "1475 Traning Loss: tensor(38068324.)\n",
      "1476 Traning Loss: tensor(38068228.)\n",
      "1477 Traning Loss: tensor(38068128.)\n",
      "1478 Traning Loss: tensor(38068040.)\n",
      "1479 Traning Loss: tensor(38067948.)\n",
      "1480 Traning Loss: tensor(38067864.)\n",
      "1481 Traning Loss: tensor(38067764.)\n",
      "1482 Traning Loss: tensor(38067672.)\n",
      "1483 Traning Loss: tensor(38067588.)\n",
      "1484 Traning Loss: tensor(38067492.)\n",
      "1485 Traning Loss: tensor(38067412.)\n",
      "1486 Traning Loss: tensor(38067324.)\n",
      "1487 Traning Loss: tensor(38067220.)\n",
      "1488 Traning Loss: tensor(38067128.)\n",
      "1489 Traning Loss: tensor(38067048.)\n",
      "1490 Traning Loss: tensor(38066964.)\n",
      "1491 Traning Loss: tensor(38066852.)\n",
      "1492 Traning Loss: tensor(38066764.)\n",
      "1493 Traning Loss: tensor(38066676.)\n",
      "1494 Traning Loss: tensor(38066596.)\n",
      "1495 Traning Loss: tensor(38066504.)\n",
      "1496 Traning Loss: tensor(38066420.)\n",
      "1497 Traning Loss: tensor(38066316.)\n",
      "1498 Traning Loss: tensor(38066228.)\n",
      "1499 Traning Loss: tensor(38066140.)\n",
      "1500 Traning Loss: tensor(38066056.)\n",
      "1501 Traning Loss: tensor(38065952.)\n",
      "1502 Traning Loss: tensor(38065868.)\n",
      "1503 Traning Loss: tensor(38065780.)\n",
      "1504 Traning Loss: tensor(38065696.)\n",
      "1505 Traning Loss: tensor(38065608.)\n",
      "1506 Traning Loss: tensor(38065524.)\n",
      "1507 Traning Loss: tensor(38073188.)\n",
      "1508 Traning Loss: tensor(38065340.)\n",
      "1509 Traning Loss: tensor(38065268.)\n",
      "1510 Traning Loss: tensor(38065196.)\n",
      "1511 Traning Loss: tensor(38065112.)\n",
      "1512 Traning Loss: tensor(38065044.)\n",
      "1513 Traning Loss: tensor(38064972.)\n",
      "1514 Traning Loss: tensor(38064892.)\n",
      "1515 Traning Loss: tensor(38064828.)\n",
      "1516 Traning Loss: tensor(38064744.)\n",
      "1517 Traning Loss: tensor(38064668.)\n",
      "1518 Traning Loss: tensor(38064596.)\n",
      "1519 Traning Loss: tensor(38064516.)\n",
      "1520 Traning Loss: tensor(38064432.)\n",
      "1521 Traning Loss: tensor(38064344.)\n",
      "1522 Traning Loss: tensor(38064256.)\n",
      "1523 Traning Loss: tensor(38064176.)\n",
      "1524 Traning Loss: tensor(38064796.)\n",
      "1525 Traning Loss: tensor(38064012.)\n",
      "1526 Traning Loss: tensor(38063920.)\n",
      "1527 Traning Loss: tensor(38063836.)\n",
      "1528 Traning Loss: tensor(38063756.)\n",
      "1529 Traning Loss: tensor(38063664.)\n",
      "1530 Traning Loss: tensor(38063576.)\n",
      "1531 Traning Loss: tensor(38063476.)\n",
      "1532 Traning Loss: tensor(38063384.)\n",
      "1533 Traning Loss: tensor(38063296.)\n",
      "1534 Traning Loss: tensor(38063200.)\n",
      "1535 Traning Loss: tensor(38063112.)\n",
      "1536 Traning Loss: tensor(38063024.)\n",
      "1537 Traning Loss: tensor(38062908.)\n",
      "1538 Traning Loss: tensor(38062820.)\n",
      "1539 Traning Loss: tensor(38062720.)\n",
      "1540 Traning Loss: tensor(38062632.)\n",
      "1541 Traning Loss: tensor(38062524.)\n",
      "1542 Traning Loss: tensor(38062432.)\n",
      "1543 Traning Loss: tensor(38062344.)\n",
      "1544 Traning Loss: tensor(38062248.)\n",
      "1545 Traning Loss: tensor(38062156.)\n",
      "1546 Traning Loss: tensor(38062064.)\n",
      "1547 Traning Loss: tensor(38061960.)\n",
      "1548 Traning Loss: tensor(38061872.)\n",
      "1549 Traning Loss: tensor(38061772.)\n",
      "1550 Traning Loss: tensor(38061684.)\n",
      "1551 Traning Loss: tensor(38061576.)\n",
      "1552 Traning Loss: tensor(38061488.)\n",
      "1553 Traning Loss: tensor(38061404.)\n",
      "1554 Traning Loss: tensor(38061304.)\n",
      "1555 Traning Loss: tensor(38104220.)\n",
      "1556 Traning Loss: tensor(38061168.)\n",
      "1557 Traning Loss: tensor(38061116.)\n",
      "1558 Traning Loss: tensor(38061080.)\n",
      "1559 Traning Loss: tensor(38061052.)\n",
      "1560 Traning Loss: tensor(38061028.)\n",
      "1561 Traning Loss: tensor(38118380.)\n",
      "1562 Traning Loss: tensor(38060984.)\n",
      "1563 Traning Loss: tensor(38060980.)\n",
      "1564 Traning Loss: tensor(38060964.)\n",
      "1565 Traning Loss: tensor(38060944.)\n",
      "1566 Traning Loss: tensor(38060912.)\n",
      "1567 Traning Loss: tensor(38060860.)\n",
      "1568 Traning Loss: tensor(38060780.)\n",
      "1569 Traning Loss: tensor(38060692.)\n",
      "1570 Traning Loss: tensor(38060596.)\n",
      "1571 Traning Loss: tensor(38060468.)\n",
      "1572 Traning Loss: tensor(38060344.)\n",
      "1573 Traning Loss: tensor(38060212.)\n",
      "1574 Traning Loss: tensor(38060084.)\n",
      "1575 Traning Loss: tensor(38059944.)\n",
      "1576 Traning Loss: tensor(38059812.)\n",
      "1577 Traning Loss: tensor(38059668.)\n",
      "1578 Traning Loss: tensor(38059536.)\n",
      "1579 Traning Loss: tensor(38059412.)\n",
      "1580 Traning Loss: tensor(38059284.)\n",
      "1581 Traning Loss: tensor(38059164.)\n",
      "1582 Traning Loss: tensor(38059040.)\n",
      "1583 Traning Loss: tensor(38058928.)\n",
      "1584 Traning Loss: tensor(38061788.)\n",
      "1585 Traning Loss: tensor(38058756.)\n",
      "1586 Traning Loss: tensor(38058704.)\n",
      "1587 Traning Loss: tensor(38058652.)\n",
      "1588 Traning Loss: tensor(38058576.)\n",
      "1589 Traning Loss: tensor(38058512.)\n",
      "1590 Traning Loss: tensor(38058428.)\n",
      "1591 Traning Loss: tensor(38058352.)\n",
      "1592 Traning Loss: tensor(38058260.)\n",
      "1593 Traning Loss: tensor(38058176.)\n",
      "1594 Traning Loss: tensor(38058088.)\n",
      "1595 Traning Loss: tensor(38057988.)\n",
      "1596 Traning Loss: tensor(38057896.)\n",
      "1597 Traning Loss: tensor(38057796.)\n",
      "1598 Traning Loss: tensor(38057676.)\n",
      "1599 Traning Loss: tensor(38057580.)\n",
      "1600 Traning Loss: tensor(38057472.)\n",
      "1601 Traning Loss: tensor(38057372.)\n",
      "1602 Traning Loss: tensor(38057248.)\n",
      "1603 Traning Loss: tensor(38057152.)\n",
      "1604 Traning Loss: tensor(38057052.)\n",
      "1605 Traning Loss: tensor(38056936.)\n",
      "1606 Traning Loss: tensor(38056840.)\n",
      "1607 Traning Loss: tensor(38056740.)\n",
      "1608 Traning Loss: tensor(38056628.)\n",
      "1609 Traning Loss: tensor(38056524.)\n",
      "1610 Traning Loss: tensor(38056428.)\n",
      "1611 Traning Loss: tensor(38056336.)\n",
      "1612 Traning Loss: tensor(38056224.)\n",
      "1613 Traning Loss: tensor(38056128.)\n",
      "1614 Traning Loss: tensor(38056036.)\n",
      "1615 Traning Loss: tensor(38055940.)\n",
      "1616 Traning Loss: tensor(38055848.)\n",
      "1617 Traning Loss: tensor(38055752.)\n",
      "1618 Traning Loss: tensor(38055648.)\n",
      "1619 Traning Loss: tensor(38055556.)\n",
      "1620 Traning Loss: tensor(38055460.)\n",
      "1621 Traning Loss: tensor(38055372.)\n",
      "1622 Traning Loss: tensor(38055256.)\n",
      "1623 Traning Loss: tensor(38055168.)\n",
      "1624 Traning Loss: tensor(38055080.)\n",
      "1625 Traning Loss: tensor(38054988.)\n",
      "1626 Traning Loss: tensor(38054900.)\n",
      "1627 Traning Loss: tensor(38054796.)\n",
      "1628 Traning Loss: tensor(38054708.)\n",
      "1629 Traning Loss: tensor(38054620.)\n",
      "1630 Traning Loss: tensor(38054528.)\n",
      "1631 Traning Loss: tensor(38086080.)\n",
      "1632 Traning Loss: tensor(38054384.)\n",
      "1633 Traning Loss: tensor(38054368.)\n",
      "1634 Traning Loss: tensor(38054352.)\n",
      "1635 Traning Loss: tensor(38054364.)\n",
      "1636 Traning Loss: tensor(38054388.)\n",
      "1637 Traning Loss: tensor(38054428.)\n",
      "1638 Traning Loss: tensor(38054440.)\n",
      "1639 Traning Loss: tensor(38054480.)\n",
      "1640 Traning Loss: tensor(38054520.)\n",
      "1641 Traning Loss: tensor(38054524.)\n",
      "1642 Traning Loss: tensor(38054496.)\n",
      "1643 Traning Loss: tensor(38054448.)\n",
      "1644 Traning Loss: tensor(38054376.)\n",
      "1645 Traning Loss: tensor(38054268.)\n",
      "1646 Traning Loss: tensor(38054136.)\n",
      "1647 Traning Loss: tensor(38053988.)\n",
      "1648 Traning Loss: tensor(38053812.)\n",
      "1649 Traning Loss: tensor(38053648.)\n",
      "1650 Traning Loss: tensor(38053480.)\n",
      "1651 Traning Loss: tensor(38053312.)\n",
      "1652 Traning Loss: tensor(38053156.)\n",
      "1653 Traning Loss: tensor(38052976.)\n",
      "1654 Traning Loss: tensor(38052840.)\n",
      "1655 Traning Loss: tensor(38052688.)\n",
      "1656 Traning Loss: tensor(38052556.)\n",
      "1657 Traning Loss: tensor(38052428.)\n",
      "1658 Traning Loss: tensor(38052288.)\n",
      "1659 Traning Loss: tensor(38052172.)\n",
      "1660 Traning Loss: tensor(38052052.)\n",
      "1661 Traning Loss: tensor(38051944.)\n",
      "1662 Traning Loss: tensor(38051836.)\n",
      "1663 Traning Loss: tensor(38051716.)\n",
      "1664 Traning Loss: tensor(38051616.)\n",
      "1665 Traning Loss: tensor(38051520.)\n",
      "1666 Traning Loss: tensor(38051420.)\n",
      "1667 Traning Loss: tensor(38051324.)\n",
      "1668 Traning Loss: tensor(38051212.)\n",
      "1669 Traning Loss: tensor(38051116.)\n",
      "1670 Traning Loss: tensor(38051016.)\n",
      "1671 Traning Loss: tensor(38050920.)\n",
      "1672 Traning Loss: tensor(38050836.)\n",
      "1673 Traning Loss: tensor(38050732.)\n",
      "1674 Traning Loss: tensor(38050636.)\n",
      "1675 Traning Loss: tensor(38058752.)\n",
      "1676 Traning Loss: tensor(38050420.)\n",
      "1677 Traning Loss: tensor(38050308.)\n",
      "1678 Traning Loss: tensor(38050196.)\n",
      "1679 Traning Loss: tensor(38050100.)\n",
      "1680 Traning Loss: tensor(38050008.)\n",
      "1681 Traning Loss: tensor(38049916.)\n",
      "1682 Traning Loss: tensor(38049808.)\n",
      "1683 Traning Loss: tensor(38049720.)\n",
      "1684 Traning Loss: tensor(38049628.)\n",
      "1685 Traning Loss: tensor(38049540.)\n",
      "1686 Traning Loss: tensor(38049460.)\n",
      "1687 Traning Loss: tensor(38049348.)\n",
      "1688 Traning Loss: tensor(38049264.)\n",
      "1689 Traning Loss: tensor(38049176.)\n",
      "1690 Traning Loss: tensor(38049088.)\n",
      "1691 Traning Loss: tensor(38049004.)\n",
      "1692 Traning Loss: tensor(38048904.)\n",
      "1693 Traning Loss: tensor(38048820.)\n",
      "1694 Traning Loss: tensor(38048728.)\n",
      "1695 Traning Loss: tensor(38048640.)\n",
      "1696 Traning Loss: tensor(38048560.)\n",
      "1697 Traning Loss: tensor(38048456.)\n",
      "1698 Traning Loss: tensor(38048372.)\n",
      "1699 Traning Loss: tensor(38048284.)\n",
      "1700 Traning Loss: tensor(38048204.)\n",
      "1701 Traning Loss: tensor(38048108.)\n",
      "1702 Traning Loss: tensor(38050832.)\n",
      "1703 Traning Loss: tensor(38047932.)\n",
      "1704 Traning Loss: tensor(38047836.)\n",
      "1705 Traning Loss: tensor(38047752.)\n",
      "1706 Traning Loss: tensor(38047664.)\n",
      "1707 Traning Loss: tensor(38047584.)\n",
      "1708 Traning Loss: tensor(38047480.)\n",
      "1709 Traning Loss: tensor(38047396.)\n",
      "1710 Traning Loss: tensor(38047308.)\n",
      "1711 Traning Loss: tensor(38047224.)\n",
      "1712 Traning Loss: tensor(38047120.)\n",
      "1713 Traning Loss: tensor(38047040.)\n",
      "1714 Traning Loss: tensor(38046952.)\n",
      "1715 Traning Loss: tensor(38046868.)\n",
      "1716 Traning Loss: tensor(38046772.)\n",
      "1717 Traning Loss: tensor(38046688.)\n",
      "1718 Traning Loss: tensor(38046596.)\n",
      "1719 Traning Loss: tensor(38046500.)\n",
      "1720 Traning Loss: tensor(38046412.)\n",
      "1721 Traning Loss: tensor(38046328.)\n",
      "1722 Traning Loss: tensor(38046232.)\n",
      "1723 Traning Loss: tensor(38046148.)\n",
      "1724 Traning Loss: tensor(38046064.)\n",
      "1725 Traning Loss: tensor(38045976.)\n",
      "1726 Traning Loss: tensor(38045888.)\n",
      "1727 Traning Loss: tensor(38045804.)\n",
      "1728 Traning Loss: tensor(38045704.)\n",
      "1729 Traning Loss: tensor(38045616.)\n",
      "1730 Traning Loss: tensor(38045528.)\n",
      "1731 Traning Loss: tensor(38045444.)\n",
      "1732 Traning Loss: tensor(38045360.)\n",
      "1733 Traning Loss: tensor(38045248.)\n",
      "1734 Traning Loss: tensor(38045176.)\n",
      "1735 Traning Loss: tensor(38045088.)\n",
      "1736 Traning Loss: tensor(38044996.)\n",
      "1737 Traning Loss: tensor(38044916.)\n",
      "1738 Traning Loss: tensor(38044812.)\n",
      "1739 Traning Loss: tensor(38044728.)\n",
      "1740 Traning Loss: tensor(38044636.)\n",
      "1741 Traning Loss: tensor(38044548.)\n",
      "1742 Traning Loss: tensor(38044456.)\n",
      "1743 Traning Loss: tensor(38044368.)\n",
      "1744 Traning Loss: tensor(38044280.)\n",
      "1745 Traning Loss: tensor(38044192.)\n",
      "1746 Traning Loss: tensor(38044108.)\n",
      "1747 Traning Loss: tensor(38044004.)\n",
      "1748 Traning Loss: tensor(38043924.)\n",
      "1749 Traning Loss: tensor(38043840.)\n",
      "1750 Traning Loss: tensor(38043748.)\n",
      "1751 Traning Loss: tensor(38043664.)\n",
      "1752 Traning Loss: tensor(38043560.)\n",
      "1753 Traning Loss: tensor(38043472.)\n",
      "1754 Traning Loss: tensor(38043388.)\n",
      "1755 Traning Loss: tensor(38043300.)\n",
      "1756 Traning Loss: tensor(38043216.)\n",
      "1757 Traning Loss: tensor(38043132.)\n",
      "1758 Traning Loss: tensor(38043028.)\n",
      "1759 Traning Loss: tensor(38042944.)\n",
      "1760 Traning Loss: tensor(38042860.)\n",
      "1761 Traning Loss: tensor(38042772.)\n",
      "1762 Traning Loss: tensor(38042668.)\n",
      "1763 Traning Loss: tensor(38042588.)\n",
      "1764 Traning Loss: tensor(38042504.)\n",
      "1765 Traning Loss: tensor(38042404.)\n",
      "1766 Traning Loss: tensor(38042332.)\n",
      "1767 Traning Loss: tensor(38042236.)\n",
      "1768 Traning Loss: tensor(38042140.)\n",
      "1769 Traning Loss: tensor(38042052.)\n",
      "1770 Traning Loss: tensor(38041968.)\n",
      "1771 Traning Loss: tensor(38041884.)\n",
      "1772 Traning Loss: tensor(38041780.)\n",
      "1773 Traning Loss: tensor(38041696.)\n",
      "1774 Traning Loss: tensor(38041612.)\n",
      "1775 Traning Loss: tensor(38041524.)\n",
      "1776 Traning Loss: tensor(38041440.)\n",
      "1777 Traning Loss: tensor(38041352.)\n",
      "1778 Traning Loss: tensor(38103008.)\n",
      "1779 Traning Loss: tensor(38041164.)\n",
      "1780 Traning Loss: tensor(38041080.)\n",
      "1781 Traning Loss: tensor(38040996.)\n",
      "1782 Traning Loss: tensor(38040912.)\n",
      "1783 Traning Loss: tensor(38040812.)\n",
      "1784 Traning Loss: tensor(38040720.)\n",
      "1785 Traning Loss: tensor(38040640.)\n",
      "1786 Traning Loss: tensor(38040544.)\n",
      "1787 Traning Loss: tensor(38040464.)\n",
      "1788 Traning Loss: tensor(38040364.)\n",
      "1789 Traning Loss: tensor(38040276.)\n",
      "1790 Traning Loss: tensor(38040192.)\n",
      "1791 Traning Loss: tensor(38040104.)\n",
      "1792 Traning Loss: tensor(38040016.)\n",
      "1793 Traning Loss: tensor(38039924.)\n",
      "1794 Traning Loss: tensor(38039836.)\n",
      "1795 Traning Loss: tensor(38039752.)\n",
      "1796 Traning Loss: tensor(38041916.)\n",
      "1797 Traning Loss: tensor(38039580.)\n",
      "1798 Traning Loss: tensor(38039496.)\n",
      "1799 Traning Loss: tensor(38039384.)\n",
      "1800 Traning Loss: tensor(38039300.)\n",
      "1801 Traning Loss: tensor(38039208.)\n",
      "1802 Traning Loss: tensor(38039128.)\n",
      "1803 Traning Loss: tensor(38039024.)\n",
      "1804 Traning Loss: tensor(38038940.)\n",
      "1805 Traning Loss: tensor(38038860.)\n",
      "1806 Traning Loss: tensor(38038768.)\n",
      "1807 Traning Loss: tensor(38038684.)\n",
      "1808 Traning Loss: tensor(38038596.)\n",
      "1809 Traning Loss: tensor(38038500.)\n",
      "1810 Traning Loss: tensor(38038412.)\n",
      "1811 Traning Loss: tensor(38038320.)\n",
      "1812 Traning Loss: tensor(38038240.)\n",
      "1813 Traning Loss: tensor(38038140.)\n",
      "1814 Traning Loss: tensor(38038056.)\n",
      "1815 Traning Loss: tensor(38037972.)\n",
      "1816 Traning Loss: tensor(38037872.)\n",
      "1817 Traning Loss: tensor(38037792.)\n",
      "1818 Traning Loss: tensor(38037704.)\n",
      "1819 Traning Loss: tensor(38037604.)\n",
      "1820 Traning Loss: tensor(38037516.)\n",
      "1821 Traning Loss: tensor(38037432.)\n",
      "1822 Traning Loss: tensor(38037344.)\n",
      "1823 Traning Loss: tensor(38037244.)\n",
      "1824 Traning Loss: tensor(38037160.)\n",
      "1825 Traning Loss: tensor(38037072.)\n",
      "1826 Traning Loss: tensor(38036988.)\n",
      "1827 Traning Loss: tensor(38036904.)\n",
      "1828 Traning Loss: tensor(38036800.)\n",
      "1829 Traning Loss: tensor(38036716.)\n",
      "1830 Traning Loss: tensor(38036628.)\n",
      "1831 Traning Loss: tensor(38036536.)\n",
      "1832 Traning Loss: tensor(38036452.)\n",
      "1833 Traning Loss: tensor(38036356.)\n",
      "1834 Traning Loss: tensor(38036272.)\n",
      "1835 Traning Loss: tensor(38036180.)\n",
      "1836 Traning Loss: tensor(38036096.)\n",
      "1837 Traning Loss: tensor(38036008.)\n",
      "1838 Traning Loss: tensor(38038708.)\n",
      "1839 Traning Loss: tensor(38035828.)\n",
      "1840 Traning Loss: tensor(38035740.)\n",
      "1841 Traning Loss: tensor(38035656.)\n",
      "1842 Traning Loss: tensor(38035660.)\n",
      "1843 Traning Loss: tensor(38035468.)\n",
      "1844 Traning Loss: tensor(38035388.)\n",
      "1845 Traning Loss: tensor(38035300.)\n",
      "1846 Traning Loss: tensor(38035888.)\n",
      "1847 Traning Loss: tensor(38035128.)\n",
      "1848 Traning Loss: tensor(38035036.)\n",
      "1849 Traning Loss: tensor(38034932.)\n",
      "1850 Traning Loss: tensor(38034860.)\n",
      "1851 Traning Loss: tensor(38034764.)\n",
      "1852 Traning Loss: tensor(38034684.)\n",
      "1853 Traning Loss: tensor(38034584.)\n",
      "1854 Traning Loss: tensor(38034496.)\n",
      "1855 Traning Loss: tensor(38034416.)\n",
      "1856 Traning Loss: tensor(38034324.)\n",
      "1857 Traning Loss: tensor(38034236.)\n",
      "1858 Traning Loss: tensor(38034156.)\n",
      "1859 Traning Loss: tensor(38034056.)\n",
      "1860 Traning Loss: tensor(38033976.)\n",
      "1861 Traning Loss: tensor(38033888.)\n",
      "1862 Traning Loss: tensor(38033800.)\n",
      "1863 Traning Loss: tensor(38033704.)\n",
      "1864 Traning Loss: tensor(38033608.)\n",
      "1865 Traning Loss: tensor(38033532.)\n",
      "1866 Traning Loss: tensor(38033448.)\n",
      "1867 Traning Loss: tensor(38033352.)\n",
      "1868 Traning Loss: tensor(38033268.)\n",
      "1869 Traning Loss: tensor(38033168.)\n",
      "1870 Traning Loss: tensor(38033080.)\n",
      "1871 Traning Loss: tensor(38032992.)\n",
      "1872 Traning Loss: tensor(38032912.)\n",
      "1873 Traning Loss: tensor(38032828.)\n",
      "1874 Traning Loss: tensor(38095520.)\n",
      "1875 Traning Loss: tensor(38032640.)\n",
      "1876 Traning Loss: tensor(38032552.)\n",
      "1877 Traning Loss: tensor(38032464.)\n",
      "1878 Traning Loss: tensor(38032380.)\n",
      "1879 Traning Loss: tensor(38032284.)\n",
      "1880 Traning Loss: tensor(38032200.)\n",
      "1881 Traning Loss: tensor(38032112.)\n",
      "1882 Traning Loss: tensor(38032024.)\n",
      "1883 Traning Loss: tensor(38031916.)\n",
      "1884 Traning Loss: tensor(38031832.)\n",
      "1885 Traning Loss: tensor(38031748.)\n",
      "1886 Traning Loss: tensor(38031668.)\n",
      "1887 Traning Loss: tensor(38031580.)\n",
      "1888 Traning Loss: tensor(38031496.)\n",
      "1889 Traning Loss: tensor(38031412.)\n",
      "1890 Traning Loss: tensor(38031312.)\n",
      "1891 Traning Loss: tensor(38031224.)\n",
      "1892 Traning Loss: tensor(38065180.)\n",
      "1893 Traning Loss: tensor(38031060.)\n",
      "1894 Traning Loss: tensor(38030964.)\n",
      "1895 Traning Loss: tensor(38030896.)\n",
      "1896 Traning Loss: tensor(38030824.)\n",
      "1897 Traning Loss: tensor(38030756.)\n",
      "1898 Traning Loss: tensor(38030684.)\n",
      "1899 Traning Loss: tensor(38030624.)\n",
      "1900 Traning Loss: tensor(38030536.)\n",
      "1901 Traning Loss: tensor(38030476.)\n",
      "1902 Traning Loss: tensor(38030420.)\n",
      "1903 Traning Loss: tensor(38030352.)\n",
      "1904 Traning Loss: tensor(38030280.)\n",
      "1905 Traning Loss: tensor(38030216.)\n",
      "1906 Traning Loss: tensor(38030156.)\n",
      "1907 Traning Loss: tensor(38030092.)\n",
      "1908 Traning Loss: tensor(38030028.)\n",
      "1909 Traning Loss: tensor(38029956.)\n",
      "1910 Traning Loss: tensor(38029868.)\n",
      "1911 Traning Loss: tensor(38029796.)\n",
      "1912 Traning Loss: tensor(38029716.)\n",
      "1913 Traning Loss: tensor(38029628.)\n",
      "1914 Traning Loss: tensor(38029536.)\n",
      "1915 Traning Loss: tensor(38029448.)\n",
      "1916 Traning Loss: tensor(38029360.)\n",
      "1917 Traning Loss: tensor(38029260.)\n",
      "1918 Traning Loss: tensor(38029168.)\n",
      "1919 Traning Loss: tensor(38029076.)\n",
      "1920 Traning Loss: tensor(38028968.)\n",
      "1921 Traning Loss: tensor(38028876.)\n",
      "1922 Traning Loss: tensor(38028772.)\n",
      "1923 Traning Loss: tensor(38028676.)\n",
      "1924 Traning Loss: tensor(38028584.)\n",
      "1925 Traning Loss: tensor(38028468.)\n",
      "1926 Traning Loss: tensor(38028376.)\n",
      "1927 Traning Loss: tensor(38028280.)\n",
      "1928 Traning Loss: tensor(38028184.)\n",
      "1929 Traning Loss: tensor(38028088.)\n",
      "1930 Traning Loss: tensor(38027976.)\n",
      "1931 Traning Loss: tensor(38027888.)\n",
      "1932 Traning Loss: tensor(38027776.)\n",
      "1933 Traning Loss: tensor(38027688.)\n",
      "1934 Traning Loss: tensor(38027596.)\n",
      "1935 Traning Loss: tensor(38027484.)\n",
      "1936 Traning Loss: tensor(38027392.)\n",
      "1937 Traning Loss: tensor(38027300.)\n",
      "1938 Traning Loss: tensor(38027208.)\n",
      "1939 Traning Loss: tensor(38027112.)\n",
      "1940 Traning Loss: tensor(38027012.)\n",
      "1941 Traning Loss: tensor(38026920.)\n",
      "1942 Traning Loss: tensor(38026824.)\n",
      "1943 Traning Loss: tensor(38026736.)\n",
      "1944 Traning Loss: tensor(38026648.)\n",
      "1945 Traning Loss: tensor(38028276.)\n",
      "1946 Traning Loss: tensor(38026464.)\n",
      "1947 Traning Loss: tensor(38026368.)\n",
      "1948 Traning Loss: tensor(38026684.)\n",
      "1949 Traning Loss: tensor(38084796.)\n",
      "1950 Traning Loss: tensor(38026132.)\n",
      "1951 Traning Loss: tensor(38026052.)\n",
      "1952 Traning Loss: tensor(38028368.)\n",
      "1953 Traning Loss: tensor(38025932.)\n",
      "1954 Traning Loss: tensor(38025876.)\n",
      "1955 Traning Loss: tensor(38025804.)\n",
      "1956 Traning Loss: tensor(38025748.)\n",
      "1957 Traning Loss: tensor(38025692.)\n",
      "1958 Traning Loss: tensor(38025624.)\n",
      "1959 Traning Loss: tensor(38025560.)\n",
      "1960 Traning Loss: tensor(38025488.)\n",
      "1961 Traning Loss: tensor(38025400.)\n",
      "1962 Traning Loss: tensor(38025324.)\n",
      "1963 Traning Loss: tensor(38042296.)\n",
      "1964 Traning Loss: tensor(38025272.)\n",
      "1965 Traning Loss: tensor(38025316.)\n",
      "1966 Traning Loss: tensor(38025336.)\n",
      "1967 Traning Loss: tensor(38025388.)\n",
      "1968 Traning Loss: tensor(38025416.)\n",
      "1969 Traning Loss: tensor(38025416.)\n",
      "1970 Traning Loss: tensor(38025400.)\n",
      "1971 Traning Loss: tensor(38025332.)\n",
      "1972 Traning Loss: tensor(38025252.)\n",
      "1973 Traning Loss: tensor(38025140.)\n",
      "1974 Traning Loss: tensor(38025008.)\n",
      "1975 Traning Loss: tensor(38024844.)\n",
      "1976 Traning Loss: tensor(38024676.)\n",
      "1977 Traning Loss: tensor(38024512.)\n",
      "1978 Traning Loss: tensor(38024340.)\n",
      "1979 Traning Loss: tensor(38024176.)\n",
      "1980 Traning Loss: tensor(38024016.)\n",
      "1981 Traning Loss: tensor(38047612.)\n",
      "1982 Traning Loss: tensor(38024020.)\n",
      "1983 Traning Loss: tensor(38024244.)\n",
      "1984 Traning Loss: tensor(38024500.)\n",
      "1985 Traning Loss: tensor(38024744.)\n",
      "1986 Traning Loss: tensor(38024968.)\n",
      "1987 Traning Loss: tensor(38025112.)\n",
      "1988 Traning Loss: tensor(38025136.)\n",
      "1989 Traning Loss: tensor(38025040.)\n",
      "1990 Traning Loss: tensor(38024836.)\n",
      "1991 Traning Loss: tensor(38024556.)\n",
      "1992 Traning Loss: tensor(38024216.)\n",
      "1993 Traning Loss: tensor(38023880.)\n",
      "1994 Traning Loss: tensor(38023544.)\n",
      "1995 Traning Loss: tensor(38023224.)\n",
      "1996 Traning Loss: tensor(38022948.)\n",
      "1997 Traning Loss: tensor(38022700.)\n",
      "1998 Traning Loss: tensor(38022480.)\n",
      "1999 Traning Loss: tensor(38022280.)\n",
      "2000 Traning Loss: tensor(38022100.)\n",
      "2001 Traning Loss: tensor(38021940.)\n",
      "2002 Traning Loss: tensor(38021768.)\n",
      "2003 Traning Loss: tensor(38021628.)\n",
      "2004 Traning Loss: tensor(38021504.)\n",
      "2005 Traning Loss: tensor(38021380.)\n",
      "2006 Traning Loss: tensor(38021252.)\n",
      "2007 Traning Loss: tensor(38021144.)\n",
      "2008 Traning Loss: tensor(38021036.)\n",
      "2009 Traning Loss: tensor(38020932.)\n",
      "2010 Traning Loss: tensor(38020828.)\n",
      "2011 Traning Loss: tensor(38020716.)\n",
      "2012 Traning Loss: tensor(38020616.)\n",
      "2013 Traning Loss: tensor(38020524.)\n",
      "2014 Traning Loss: tensor(38020428.)\n",
      "2015 Traning Loss: tensor(38020336.)\n",
      "2016 Traning Loss: tensor(38020232.)\n",
      "2017 Traning Loss: tensor(38020132.)\n",
      "2018 Traning Loss: tensor(38020044.)\n",
      "2019 Traning Loss: tensor(38019948.)\n",
      "2020 Traning Loss: tensor(38082316.)\n",
      "2021 Traning Loss: tensor(38019760.)\n",
      "2022 Traning Loss: tensor(38019672.)\n",
      "2023 Traning Loss: tensor(38019592.)\n",
      "2024 Traning Loss: tensor(38019504.)\n",
      "2025 Traning Loss: tensor(38019412.)\n",
      "2026 Traning Loss: tensor(38019316.)\n",
      "2027 Traning Loss: tensor(38019228.)\n",
      "2028 Traning Loss: tensor(38019144.)\n",
      "2029 Traning Loss: tensor(38019052.)\n",
      "2030 Traning Loss: tensor(38018972.)\n",
      "2031 Traning Loss: tensor(38018888.)\n",
      "2032 Traning Loss: tensor(38018784.)\n",
      "2033 Traning Loss: tensor(38018700.)\n",
      "2034 Traning Loss: tensor(38018604.)\n",
      "2035 Traning Loss: tensor(38018520.)\n",
      "2036 Traning Loss: tensor(38018436.)\n",
      "2037 Traning Loss: tensor(38018332.)\n",
      "2038 Traning Loss: tensor(38018248.)\n",
      "2039 Traning Loss: tensor(38018156.)\n",
      "2040 Traning Loss: tensor(38018068.)\n",
      "2041 Traning Loss: tensor(38017980.)\n",
      "2042 Traning Loss: tensor(38017880.)\n",
      "2043 Traning Loss: tensor(38017792.)\n",
      "2044 Traning Loss: tensor(38017704.)\n",
      "2045 Traning Loss: tensor(38017620.)\n",
      "2046 Traning Loss: tensor(38017532.)\n",
      "2047 Traning Loss: tensor(38017432.)\n",
      "2048 Traning Loss: tensor(38017348.)\n",
      "2049 Traning Loss: tensor(38017256.)\n",
      "2050 Traning Loss: tensor(38017176.)\n",
      "2051 Traning Loss: tensor(38017080.)\n",
      "2052 Traning Loss: tensor(38016988.)\n",
      "2053 Traning Loss: tensor(38016888.)\n",
      "2054 Traning Loss: tensor(38016816.)\n",
      "2055 Traning Loss: tensor(38016720.)\n",
      "2056 Traning Loss: tensor(38016632.)\n",
      "2057 Traning Loss: tensor(38016532.)\n",
      "2058 Traning Loss: tensor(38016444.)\n",
      "2059 Traning Loss: tensor(38016356.)\n",
      "2060 Traning Loss: tensor(38016272.)\n",
      "2061 Traning Loss: tensor(38016184.)\n",
      "2062 Traning Loss: tensor(38016084.)\n",
      "2063 Traning Loss: tensor(38015996.)\n",
      "2064 Traning Loss: tensor(38015912.)\n",
      "2065 Traning Loss: tensor(38015824.)\n",
      "2066 Traning Loss: tensor(38015728.)\n",
      "2067 Traning Loss: tensor(38015640.)\n",
      "2068 Traning Loss: tensor(38015544.)\n",
      "2069 Traning Loss: tensor(38015464.)\n",
      "2070 Traning Loss: tensor(38015372.)\n",
      "2071 Traning Loss: tensor(38015284.)\n",
      "2072 Traning Loss: tensor(38015188.)\n",
      "2073 Traning Loss: tensor(38015100.)\n",
      "2074 Traning Loss: tensor(38015016.)\n",
      "2075 Traning Loss: tensor(38015168.)\n",
      "2076 Traning Loss: tensor(38014828.)\n",
      "2077 Traning Loss: tensor(38014748.)\n",
      "2078 Traning Loss: tensor(38014656.)\n",
      "2079 Traning Loss: tensor(38014568.)\n",
      "2080 Traning Loss: tensor(38014480.)\n",
      "2081 Traning Loss: tensor(38014392.)\n",
      "2082 Traning Loss: tensor(38014316.)\n",
      "2083 Traning Loss: tensor(38014212.)\n",
      "2084 Traning Loss: tensor(38014124.)\n",
      "2085 Traning Loss: tensor(38014028.)\n",
      "2086 Traning Loss: tensor(38013928.)\n",
      "2087 Traning Loss: tensor(38013840.)\n",
      "2088 Traning Loss: tensor(38013760.)\n",
      "2089 Traning Loss: tensor(38013676.)\n",
      "2090 Traning Loss: tensor(38013584.)\n",
      "2091 Traning Loss: tensor(38013500.)\n",
      "2092 Traning Loss: tensor(38013412.)\n",
      "2093 Traning Loss: tensor(38013312.)\n",
      "2094 Traning Loss: tensor(38013228.)\n",
      "2095 Traning Loss: tensor(38013136.)\n",
      "2096 Traning Loss: tensor(38013052.)\n",
      "2097 Traning Loss: tensor(38012952.)\n",
      "2098 Traning Loss: tensor(38012872.)\n",
      "2099 Traning Loss: tensor(38012784.)\n",
      "2100 Traning Loss: tensor(38012688.)\n",
      "2101 Traning Loss: tensor(38012604.)\n",
      "2102 Traning Loss: tensor(38012504.)\n",
      "2103 Traning Loss: tensor(38012420.)\n",
      "2104 Traning Loss: tensor(38026668.)\n",
      "2105 Traning Loss: tensor(38012268.)\n",
      "2106 Traning Loss: tensor(38012212.)\n",
      "2107 Traning Loss: tensor(38012148.)\n",
      "2108 Traning Loss: tensor(38012104.)\n",
      "2109 Traning Loss: tensor(38012068.)\n",
      "2110 Traning Loss: tensor(38012036.)\n",
      "2111 Traning Loss: tensor(38012012.)\n",
      "2112 Traning Loss: tensor(38011976.)\n",
      "2113 Traning Loss: tensor(38011960.)\n",
      "2114 Traning Loss: tensor(38011936.)\n",
      "2115 Traning Loss: tensor(38011912.)\n",
      "2116 Traning Loss: tensor(38011876.)\n",
      "2117 Traning Loss: tensor(38011824.)\n",
      "2118 Traning Loss: tensor(38011780.)\n",
      "2119 Traning Loss: tensor(38011716.)\n",
      "2120 Traning Loss: tensor(38011648.)\n",
      "2121 Traning Loss: tensor(38011564.)\n",
      "2122 Traning Loss: tensor(38011476.)\n",
      "2123 Traning Loss: tensor(38011360.)\n",
      "2124 Traning Loss: tensor(38011252.)\n",
      "2125 Traning Loss: tensor(38011140.)\n",
      "2126 Traning Loss: tensor(38011012.)\n",
      "2127 Traning Loss: tensor(38010880.)\n",
      "2128 Traning Loss: tensor(38010764.)\n",
      "2129 Traning Loss: tensor(38010640.)\n",
      "2130 Traning Loss: tensor(38010516.)\n",
      "2131 Traning Loss: tensor(38010396.)\n",
      "2132 Traning Loss: tensor(38010280.)\n",
      "2133 Traning Loss: tensor(38010136.)\n",
      "2134 Traning Loss: tensor(38010032.)\n",
      "2135 Traning Loss: tensor(38009916.)\n",
      "2136 Traning Loss: tensor(38009804.)\n",
      "2137 Traning Loss: tensor(38009684.)\n",
      "2138 Traning Loss: tensor(38009576.)\n",
      "2139 Traning Loss: tensor(38009472.)\n",
      "2140 Traning Loss: tensor(38009368.)\n",
      "2141 Traning Loss: tensor(38009260.)\n",
      "2142 Traning Loss: tensor(38009168.)\n",
      "2143 Traning Loss: tensor(38009052.)\n",
      "2144 Traning Loss: tensor(38008956.)\n",
      "2145 Traning Loss: tensor(38008856.)\n",
      "2146 Traning Loss: tensor(38008764.)\n",
      "2147 Traning Loss: tensor(38008672.)\n",
      "2148 Traning Loss: tensor(38008552.)\n",
      "2149 Traning Loss: tensor(38008472.)\n",
      "2150 Traning Loss: tensor(38008368.)\n",
      "2151 Traning Loss: tensor(38008276.)\n",
      "2152 Traning Loss: tensor(38008184.)\n",
      "2153 Traning Loss: tensor(38008076.)\n",
      "2154 Traning Loss: tensor(38007988.)\n",
      "2155 Traning Loss: tensor(38007892.)\n",
      "2156 Traning Loss: tensor(38007804.)\n",
      "2157 Traning Loss: tensor(38007712.)\n",
      "2158 Traning Loss: tensor(38007608.)\n",
      "2159 Traning Loss: tensor(38007520.)\n",
      "2160 Traning Loss: tensor(38007428.)\n",
      "2161 Traning Loss: tensor(38007340.)\n",
      "2162 Traning Loss: tensor(38007252.)\n",
      "2163 Traning Loss: tensor(38007148.)\n",
      "2164 Traning Loss: tensor(38007060.)\n",
      "2165 Traning Loss: tensor(38006960.)\n",
      "2166 Traning Loss: tensor(38006880.)\n",
      "2167 Traning Loss: tensor(38006796.)\n",
      "2168 Traning Loss: tensor(38006696.)\n",
      "2169 Traning Loss: tensor(38072864.)\n",
      "2170 Traning Loss: tensor(38006512.)\n",
      "2171 Traning Loss: tensor(38006428.)\n",
      "2172 Traning Loss: tensor(38006340.)\n",
      "2173 Traning Loss: tensor(38006256.)\n",
      "2174 Traning Loss: tensor(38006156.)\n",
      "2175 Traning Loss: tensor(38006064.)\n",
      "2176 Traning Loss: tensor(38005976.)\n",
      "2177 Traning Loss: tensor(38005896.)\n",
      "2178 Traning Loss: tensor(38005792.)\n",
      "2179 Traning Loss: tensor(38005708.)\n",
      "2180 Traning Loss: tensor(38005620.)\n",
      "2181 Traning Loss: tensor(38005536.)\n",
      "2182 Traning Loss: tensor(38005436.)\n",
      "2183 Traning Loss: tensor(38005348.)\n",
      "2184 Traning Loss: tensor(38005256.)\n",
      "2185 Traning Loss: tensor(38005172.)\n",
      "2186 Traning Loss: tensor(38005076.)\n",
      "2187 Traning Loss: tensor(38004988.)\n",
      "2188 Traning Loss: tensor(38004904.)\n",
      "2189 Traning Loss: tensor(38004800.)\n",
      "2190 Traning Loss: tensor(38004720.)\n",
      "2191 Traning Loss: tensor(38004632.)\n",
      "2192 Traning Loss: tensor(38004548.)\n",
      "2193 Traning Loss: tensor(38004460.)\n",
      "2194 Traning Loss: tensor(38004360.)\n",
      "2195 Traning Loss: tensor(38004272.)\n",
      "2196 Traning Loss: tensor(38004188.)\n",
      "2197 Traning Loss: tensor(38004096.)\n",
      "2198 Traning Loss: tensor(38003996.)\n",
      "2199 Traning Loss: tensor(38003908.)\n",
      "2200 Traning Loss: tensor(38003824.)\n",
      "2201 Traning Loss: tensor(38003724.)\n",
      "2202 Traning Loss: tensor(38003644.)\n",
      "2203 Traning Loss: tensor(38003560.)\n",
      "2204 Traning Loss: tensor(38003456.)\n",
      "2205 Traning Loss: tensor(38003372.)\n",
      "2206 Traning Loss: tensor(38003280.)\n",
      "2207 Traning Loss: tensor(38003196.)\n",
      "2208 Traning Loss: tensor(38003100.)\n",
      "2209 Traning Loss: tensor(38003012.)\n",
      "2210 Traning Loss: tensor(38002924.)\n",
      "2211 Traning Loss: tensor(38002832.)\n",
      "2212 Traning Loss: tensor(38002748.)\n",
      "2213 Traning Loss: tensor(38002668.)\n",
      "2214 Traning Loss: tensor(38002564.)\n",
      "2215 Traning Loss: tensor(38002480.)\n",
      "2216 Traning Loss: tensor(38002388.)\n",
      "2217 Traning Loss: tensor(38002304.)\n",
      "2218 Traning Loss: tensor(38002196.)\n",
      "2219 Traning Loss: tensor(38002112.)\n",
      "2220 Traning Loss: tensor(38002032.)\n",
      "2221 Traning Loss: tensor(38001940.)\n",
      "2222 Traning Loss: tensor(38001856.)\n",
      "2223 Traning Loss: tensor(38001768.)\n",
      "2224 Traning Loss: tensor(38001684.)\n",
      "2225 Traning Loss: tensor(38001580.)\n",
      "2226 Traning Loss: tensor(38001492.)\n",
      "2227 Traning Loss: tensor(38001408.)\n",
      "2228 Traning Loss: tensor(38001320.)\n",
      "2229 Traning Loss: tensor(38001220.)\n",
      "2230 Traning Loss: tensor(38001136.)\n",
      "2231 Traning Loss: tensor(38001044.)\n",
      "2232 Traning Loss: tensor(38000960.)\n",
      "2233 Traning Loss: tensor(38000872.)\n",
      "2234 Traning Loss: tensor(38000768.)\n",
      "2235 Traning Loss: tensor(38000680.)\n",
      "2236 Traning Loss: tensor(38000592.)\n",
      "2237 Traning Loss: tensor(38000512.)\n",
      "2238 Traning Loss: tensor(38000424.)\n",
      "2239 Traning Loss: tensor(38000328.)\n",
      "2240 Traning Loss: tensor(38000240.)\n",
      "2241 Traning Loss: tensor(38000232.)\n",
      "2242 Traning Loss: tensor(38000064.)\n",
      "2243 Traning Loss: tensor(37999980.)\n",
      "2244 Traning Loss: tensor(37999884.)\n",
      "2245 Traning Loss: tensor(37999796.)\n",
      "2246 Traning Loss: tensor(37999708.)\n",
      "2247 Traning Loss: tensor(37999624.)\n",
      "2248 Traning Loss: tensor(37999536.)\n",
      "2249 Traning Loss: tensor(37999440.)\n",
      "2250 Traning Loss: tensor(37999344.)\n",
      "2251 Traning Loss: tensor(37999260.)\n",
      "2252 Traning Loss: tensor(37999168.)\n",
      "2253 Traning Loss: tensor(37999088.)\n",
      "2254 Traning Loss: tensor(37998988.)\n",
      "2255 Traning Loss: tensor(37998900.)\n",
      "2256 Traning Loss: tensor(37998816.)\n",
      "2257 Traning Loss: tensor(37998728.)\n",
      "2258 Traning Loss: tensor(37998640.)\n",
      "2259 Traning Loss: tensor(37998540.)\n",
      "2260 Traning Loss: tensor(37998452.)\n",
      "2261 Traning Loss: tensor(37998372.)\n",
      "2262 Traning Loss: tensor(37998280.)\n",
      "2263 Traning Loss: tensor(37998196.)\n",
      "2264 Traning Loss: tensor(37998092.)\n",
      "2265 Traning Loss: tensor(37998004.)\n",
      "2266 Traning Loss: tensor(37997928.)\n",
      "2267 Traning Loss: tensor(37997828.)\n",
      "2268 Traning Loss: tensor(37997744.)\n",
      "2269 Traning Loss: tensor(37997644.)\n",
      "2270 Traning Loss: tensor(37997560.)\n",
      "2271 Traning Loss: tensor(37997472.)\n",
      "2272 Traning Loss: tensor(37997384.)\n",
      "2273 Traning Loss: tensor(37997300.)\n",
      "2274 Traning Loss: tensor(37997216.)\n",
      "2275 Traning Loss: tensor(37997116.)\n",
      "2276 Traning Loss: tensor(37997036.)\n",
      "2277 Traning Loss: tensor(37996944.)\n",
      "2278 Traning Loss: tensor(37996860.)\n",
      "2279 Traning Loss: tensor(37996756.)\n",
      "2280 Traning Loss: tensor(37996692.)\n",
      "2281 Traning Loss: tensor(37996592.)\n",
      "2282 Traning Loss: tensor(37996492.)\n",
      "2283 Traning Loss: tensor(37996408.)\n",
      "2284 Traning Loss: tensor(37996324.)\n",
      "2285 Traning Loss: tensor(37996224.)\n",
      "2286 Traning Loss: tensor(37996132.)\n",
      "2287 Traning Loss: tensor(37996048.)\n",
      "2288 Traning Loss: tensor(37995968.)\n",
      "2289 Traning Loss: tensor(37995860.)\n",
      "2290 Traning Loss: tensor(37995772.)\n",
      "2291 Traning Loss: tensor(37995688.)\n",
      "2292 Traning Loss: tensor(37995608.)\n",
      "2293 Traning Loss: tensor(37995520.)\n",
      "2294 Traning Loss: tensor(37995432.)\n",
      "2295 Traning Loss: tensor(37995336.)\n",
      "2296 Traning Loss: tensor(37995240.)\n",
      "2297 Traning Loss: tensor(37995152.)\n",
      "2298 Traning Loss: tensor(37995076.)\n",
      "2299 Traning Loss: tensor(37994984.)\n",
      "2300 Traning Loss: tensor(37994884.)\n",
      "2301 Traning Loss: tensor(37994796.)\n",
      "2302 Traning Loss: tensor(37994716.)\n",
      "2303 Traning Loss: tensor(37994620.)\n",
      "2304 Traning Loss: tensor(37994540.)\n",
      "2305 Traning Loss: tensor(37994440.)\n",
      "2306 Traning Loss: tensor(37994356.)\n",
      "2307 Traning Loss: tensor(37994264.)\n",
      "2308 Traning Loss: tensor(37994180.)\n",
      "2309 Traning Loss: tensor(37994096.)\n",
      "2310 Traning Loss: tensor(37994000.)\n",
      "2311 Traning Loss: tensor(37993912.)\n",
      "2312 Traning Loss: tensor(37993828.)\n",
      "2313 Traning Loss: tensor(37993736.)\n",
      "2314 Traning Loss: tensor(37993648.)\n",
      "2315 Traning Loss: tensor(37993552.)\n",
      "2316 Traning Loss: tensor(37993468.)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/utkarshpratiush/RA_IISc/Inverse-problem-pde/PINN/Lotka_volterra_pinn.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/utkarshpratiush/RA_IISc/Inverse-problem-pde/PINN/Lotka_volterra_pinn.ipynb#ch0000006?line=41'>42</a>\u001b[0m loss \u001b[39m=\u001b[39m mse_u_bc \u001b[39m+\u001b[39m mse_v_bc  \u001b[39m+\u001b[39m mse_f_1 \u001b[39m+\u001b[39m mse_f_2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/utkarshpratiush/RA_IISc/Inverse-problem-pde/PINN/Lotka_volterra_pinn.ipynb#ch0000006?line=44'>45</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward() \u001b[39m# This is for computing gradients using backward propagation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/utkarshpratiush/RA_IISc/Inverse-problem-pde/PINN/Lotka_volterra_pinn.ipynb#ch0000006?line=45'>46</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep() \u001b[39m# This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/utkarshpratiush/RA_IISc/Inverse-problem-pde/PINN/Lotka_volterra_pinn.ipynb#ch0000006?line=47'>48</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/utkarshpratiush/RA_IISc/Inverse-problem-pde/PINN/Lotka_volterra_pinn.ipynb#ch0000006?line=48'>49</a>\u001b[0m \t\u001b[39mprint\u001b[39m(epoch,\u001b[39m\"\u001b[39m\u001b[39mTraning Loss:\u001b[39m\u001b[39m\"\u001b[39m,loss\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m     <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=137'>138</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=138'>139</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=140'>141</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=141'>142</a>\u001b[0m            grads,\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=142'>143</a>\u001b[0m            exp_avgs,\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=143'>144</a>\u001b[0m            exp_avg_sqs,\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=144'>145</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=145'>146</a>\u001b[0m            state_steps,\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=146'>147</a>\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=147'>148</a>\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=148'>149</a>\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=149'>150</a>\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=150'>151</a>\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=151'>152</a>\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=152'>153</a>\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/adam.py?line=153'>154</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/_functional.py:98\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/_functional.py?line=95'>96</a>\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/_functional.py?line=96'>97</a>\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m---> <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/_functional.py?line=97'>98</a>\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[1;32m     <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/_functional.py?line=98'>99</a>\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/_functional.py?line=99'>100</a>\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/utkarshpratiush/opt/anaconda3/envs/pinn/lib/python3.8/site-packages/torch/optim/_functional.py?line=100'>101</a>\u001b[0m     torch\u001b[39m.\u001b[39mmaximum(max_exp_avg_sqs[i], exp_avg_sq, out\u001b[39m=\u001b[39mmax_exp_avg_sqs[i])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### (3) Training / Fitting\n",
    "iterations = 199999\n",
    "epoch = 0\n",
    "loss =  torch.tensor(10000)\n",
    "#previous_validation_loss = 99999999.0\n",
    "#for epoch in range(iterations):\n",
    "while True :\n",
    "    epoch += 1\n",
    "    optimizer.zero_grad() # to make the gradients zero\n",
    "    \n",
    "    # Loss based on boundary conditions\n",
    "    #pt_x_bc = Variable(torch.Tensor(x_bc).float(), requires_grad=False).to(device)\n",
    "    pt_t_bc = Variable(torch.Tensor(t_bc).float(), requires_grad=False).to(device)\n",
    "    pt_u_bc = Variable(torch.Tensor(u_bc).float(), requires_grad=False).to(device)\n",
    "    pt_v_bc = Variable(torch.Tensor(v_bc).float(), requires_grad=False).to(device)\n",
    "\n",
    "    \n",
    "    net_u_bc_out = net_x(pt_t_bc) # output of u(x,t)\n",
    "    mse_u_bc = mse_cost_function(net_u_bc_out, pt_u_bc)\n",
    "\n",
    "    net_v_bc_out = net_y(pt_t_bc) # output of u(x,t)\n",
    "    mse_v_bc = mse_cost_function(net_v_bc_out, pt_v_bc)\n",
    "    \n",
    "    # Loss based on PDE\n",
    "    #x_collocation = np.random.uniform(low=0.0, high=2.0, size=(500,1))\n",
    "    t_collocation = np.random.uniform(low=0.0, high=1000.0, size=(500,1))\n",
    "    all_zeros_1 = np.zeros((500,1))\n",
    "    all_zeros_2 = np.zeros((500,1))\n",
    "    \n",
    "    \n",
    "    #pt_x_collocation = Variable(torch.Tensor(x_collocation).float(), requires_grad=True).to(device)\n",
    "    pt_t_collocation = Variable(torch.Tensor(t_collocation).float(), requires_grad=True).to(device)\n",
    "    pt_all_zeros_1 = Variable(torch.Tensor(all_zeros_1).float(), requires_grad=False).to(device)\n",
    "    pt_all_zeros_2 = Variable(torch.Tensor(all_zeros_1).float(), requires_grad=False).to(device)\n",
    "\n",
    "    \n",
    "    pde_1, pde_2 = f(pt_t_collocation, net_x, net_y) # output of the differential eqn\n",
    "    mse_f_1 = mse_cost_function(pde_1, pt_all_zeros_1)\n",
    "    mse_f_2 = mse_cost_function(pde_2, pt_all_zeros_2)\n",
    "    \n",
    "    # Combining the loss functions\n",
    "    loss = mse_u_bc + mse_v_bc  + mse_f_1 + mse_f_2\n",
    "    \n",
    "    \n",
    "    loss.backward() # This is for computing gradients using backward propagation\n",
    "    optimizer.step() # This is equivalent to : theta_new = theta_old - alpha * derivative of J w.r.t theta\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "    \tprint(epoch,\"Traning Loss:\",loss.data)\n",
    "\n",
    "    if (loss.item() <= 0.00005):\n",
    "                        break\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(10000).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c2f93965a3ca292b7715d96980092ba920b6b7cd48889a529ea1962f3df167c4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pinn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
